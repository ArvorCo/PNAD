{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PNADC Exploration (NPV-ready)\n",
    "\n",
    "Este notebook normaliza o fluxo: (1) busca IPCA do BCB, (2) ajusta rendimentos para valor presente (m\u00eas mais recente dispon\u00edvel), (3) adiciona coluna em sal\u00e1rios m\u00ednimos, (4) persiste Parquet/CSV NPV e (5) conecta DuckDB usando o DataFrame ajustado.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Imports & display configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "from urllib.request import urlopen, Request\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 160)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPV Setup (IPCA BCB) \u2014 deflate VD4020 ao valor presente\n",
    "\n",
    "- Busca IPCA mensal (varia\u00e7\u00e3o %) no BCB (SGS 433).\n",
    "- Comp\u00f5e \u00edndice encadeado e rebaseia no m\u00eas-alvo mais recente.\n",
    "- Calcula `factor_to_target` = \u00edndice(alvo) / \u00edndice(m\u00eas). A base cancela.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "INCOME_COL = 'VD4020__rendim_efetivo_qq_trabalho'\n",
    "MIN_WAGE = 1518.0  # sal\u00e1rio m\u00ednimo para converter em 'sal\u00e1rios m\u00ednimos'\n",
    "\n",
    "def fetch_ipca_bcb_variation(series: int = 433) -> pd.DataFrame:\n",
    "    url = f'https://api.bcb.gov.br/dados/serie/bcdata.sgs.{series}/dados?formato=json'\n",
    "    req = Request(url, headers={'User-Agent': 'pnad-npv/1.0'})\n",
    "    with urlopen(req, timeout=60) as resp:\n",
    "        data = resp.read().decode('utf-8')\n",
    "    items = json.loads(data)\n",
    "    out = []\n",
    "    for it in items:\n",
    "        d = str(it.get('data', ''))\n",
    "        parts = d.split('/')\n",
    "        if len(parts) == 2:\n",
    "            m, y = int(parts[0]), int(parts[1])\n",
    "        else:\n",
    "            m, y = int(parts[1]), int(parts[2])\n",
    "        val = float(str(it.get('valor', '')).replace(',', '.'))\n",
    "        out.append((f'{y}-{m:02d}', val))\n",
    "    df = pd.DataFrame(out, columns=['ym', 'pct_month']).sort_values('ym').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def build_index_and_factors(df_pct: pd.DataFrame) -> pd.DataFrame:\n",
    "    idx = (1.0 + df_pct['pct_month'] / 100.0).cumprod()\n",
    "    out = pd.DataFrame({'ym': df_pct['ym'], 'index': idx})\n",
    "    target_ym = out['ym'].iloc[-1]\n",
    "    target_idx = float(out.loc[out['ym'] == target_ym, 'index'].iloc[0])\n",
    "    out['factor_to_target'] = target_idx / out['index']\n",
    "    return out, target_ym\n",
    "\n",
    "ipca_pct = fetch_ipca_bcb_variation()\n",
    "ipca_idx, TARGET_YM = build_index_and_factors(ipca_pct)\n",
    "display({'target_month': TARGET_YM})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, apply NPV adjustments, save outputs, and setup DuckDB\n",
    "\n",
    "- Resolve `data/base_labeled.{parquet,csv}` subindo diret\u00f3rios.\n",
    "- Deriva `ym` de Ano/Trimestre (\u00faltimo m\u00eas do trimestre).\n",
    "- Faz merge com fatores e cria colunas deflacionadas e em sal\u00e1rios m\u00ednimos.\n",
    "- Salva `data/base_labeled_npv.{parquet,csv}`.\n",
    "- Registra `df` como view `base` no DuckDB.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_in_data(fn: str) -> Path | None:\n",
    "    for base in [Path.cwd()] + list(Path.cwd().parents):\n",
    "        cand = base / 'data' / fn\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "# Load data\n",
    "parquet_file = find_in_data('base_labeled.parquet')\n",
    "csv_file = find_in_data('base_labeled.csv')\n",
    "if parquet_file:\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "elif csv_file:\n",
    "    df = pd.read_csv(csv_file)\n",
    "else:\n",
    "    raise FileNotFoundError('Missing base_labeled.{parquet,csv} under data/')\n",
    "display({'resolved_parquet': str(parquet_file) if parquet_file else None, 'resolved_csv': str(csv_file) if csv_file else None})\n",
    "\n",
    "# Create year-month column from Ano and Trimestre\n",
    "Q2M = {1: '03', 2: '06', 3: '09', 4: '12'}\n",
    "def to_int(x):\n",
    "    try:\n",
    "        return int(str(x).strip())\n",
    "    except Exception:\n",
    "        return None\n",
    "year_col = next((c for c in df.columns if c.startswith('Ano__')), 'Ano__ano_de_referncia')\n",
    "quarter_col = next((c for c in df.columns if c.startswith('Trimestre__')), 'Trimestre__trimestre_de_referncia')\n",
    "df['ym'] = df[year_col].map(lambda x: str(x).split('.')[0]) + '-' + df[quarter_col].map(lambda x: Q2M.get(to_int(x), '12'))\n",
    "\n",
    "# Apply NPV adjustments\n",
    "df = df.merge(ipca_idx[['ym','factor_to_target']], on='ym', how='left')\n",
    "npv_col = f\"{INCOME_COL}_{TARGET_YM.replace('-', '')}\"\n",
    "mw_col = f'{INCOME_COL}_mw'\n",
    "df[npv_col] = pd.to_numeric(df[INCOME_COL], errors='coerce') * df['factor_to_target']\n",
    "df[mw_col] = df[npv_col] / float(MIN_WAGE)\n",
    "display(df[[INCOME_COL, npv_col, mw_col]].describe(include='all'))\n",
    "\n",
    "# Save NPV-adjusted data\n",
    "out_dir = (parquet_file or csv_file).parent\n",
    "npv_parquet = out_dir / 'base_labeled_npv.parquet'\n",
    "df.to_parquet(npv_parquet, index=False)\n",
    "npv_csv = out_dir / 'base_labeled_npv.csv'\n",
    "try:\n",
    "    df.to_csv(npv_csv, index=False)\n",
    "except Exception:\n",
    "    pass\n",
    "display({'saved_parquet': str(npv_parquet), 'saved_csv': str(npv_csv)})\n",
    "\n",
    "# Setup DuckDB connection\n",
    "con = duckdb.connect(database=':memory:')\n",
    "try:\n",
    "    con.register('base', df)\n",
    "    source = 'df'\n",
    "except Exception:\n",
    "    con.sql(f\"CREATE OR REPLACE VIEW base AS SELECT * FROM read_parquet('{str(npv_parquet).replace(chr(39),chr(39)+chr(39))}')\") \n",
    "    source = str(npv_parquet)\n",
    "display({'duck_source': source})\n",
    "con.sql('select count(*) as rows from base').df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Individual Income Analysis - Minimum Wage Bands\n\nAn\u00e1lise da distribui\u00e7\u00e3o de renda individual em faixas de sal\u00e1rios m\u00ednimos."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Individual Income Band Analysis - Version 1 (0-2, 2-5, 5+ MW)\nmw_col = f'{INCOME_COL}_mw'\n\n# Filter valid income data (exclude nulls and zeros)\ndf_income = df[df[mw_col].notna() & (df[mw_col] > 0)].copy()\n\n# Create income bands\ndef categorize_income_v1(mw):\n    if mw < 2:\n        return '0-2 MW'\n    elif mw < 5:\n        return '2-5 MW'\n    else:\n        return '5+ MW'\n\ndf_income['income_band_v1'] = df_income[mw_col].apply(categorize_income_v1)\n\n# Calculate distribution\nincome_dist_v1 = df_income['income_band_v1'].value_counts().sort_index()\nincome_pct_v1 = (income_dist_v1 / income_dist_v1.sum() * 100).round(2)\n\n# Display results\ndisplay({\n    'Total people with income': len(df_income),\n    'Income bands (0-2, 2-5, 5+ MW)': pd.DataFrame({\n        'Count': income_dist_v1,\n        'Percentage': income_pct_v1\n    })\n})\n\n# Summary statistics by band\ndisplay(df_income.groupby('income_band_v1')[mw_col].describe().round(2))"
  },
  {
   "cell_type": "code",
   "id": "3pdiph5vf3",
   "source": "# Individual Income Band Analysis - Version 2 (0-2, 2-5, 5-10, 10+ MW)\ndef categorize_income_v2(mw):\n    if mw < 2:\n        return '0-2 MW'\n    elif mw < 5:\n        return '2-5 MW'\n    elif mw < 10:\n        return '5-10 MW'\n    else:\n        return '10+ MW'\n\ndf_income['income_band_v2'] = df_income[mw_col].apply(categorize_income_v2)\n\n# Calculate distribution\nincome_dist_v2 = df_income['income_band_v2'].value_counts().sort_index()\nincome_pct_v2 = (income_dist_v2 / income_dist_v2.sum() * 100).round(2)\n\n# Display results\ndisplay({\n    'Income bands (0-2, 2-5, 5-10, 10+ MW)': pd.DataFrame({\n        'Count': income_dist_v2,\n        'Percentage': income_pct_v2\n    })\n})\n\n# Summary statistics by band\ndisplay(df_income.groupby('income_band_v2')[mw_col].describe().round(2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ft7aa0xrba",
   "source": "## Household Income Analysis\n\nAgrega\u00e7\u00e3o de renda por domic\u00edlio e an\u00e1lise das faixas de renda familiar.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0dgset1xefv7",
   "source": "# Household Income Aggregation\n# Create household ID (dom_id)\ndf['dom_id'] = (df['Ano__ano_de_referncia'].astype(str) + '_' +\n                df['Trimestre__trimestre_de_referncia'].astype(str) + '_' +\n                df['UPA__unidade_primria_de_amostragem'].astype(str) + '_' +\n                df['V1008__nmero_de_seleo_do_domiclio'].astype(str))\n\n# Aggregate income by household\nhousehold_income = df.groupby('dom_id').agg({\n    mw_col: 'sum',  # Total household income in MW\n    npv_col: 'sum',  # Total household income NPV-adjusted\n    'VD2003__nmero_de_componentes_do_domic': 'first',  # Number of household members\n    'UF_label': 'first',\n    'Capital_label': 'first',\n    'RM_RIDE_label': 'first'\n}).rename(columns={\n    mw_col: 'household_income_mw',\n    npv_col: 'household_income_npv',\n    'VD2003__nmero_de_componentes_do_domic': 'household_size'\n})\n\n# Calculate per capita income\nhousehold_income['per_capita_income_mw'] = household_income['household_income_mw'] / household_income['household_size']\n\n# Filter valid households\nhousehold_income = household_income[household_income['household_income_mw'].notna() & \n                                    (household_income['household_income_mw'] > 0)]\n\ndisplay({\n    'Total households': len(household_income),\n    'Average household size': household_income['household_size'].mean().round(2),\n    'Household income statistics (MW)': household_income['household_income_mw'].describe().round(2)\n})",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "lmtqak7ukuf",
   "source": "# Household Income Bands Analysis\ndef categorize_household_income(mw):\n    if mw < 2:\n        return '0-2 MW'\n    elif mw < 5:\n        return '2-5 MW'\n    elif mw < 10:\n        return '5-10 MW'\n    else:\n        return '10+ MW'\n\nhousehold_income['income_band'] = household_income['household_income_mw'].apply(categorize_household_income)\n\n# Calculate distribution\nhousehold_dist = household_income['income_band'].value_counts().sort_index()\nhousehold_pct = (household_dist / household_dist.sum() * 100).round(2)\n\ndisplay({\n    'Household income bands': pd.DataFrame({\n        'Count': household_dist,\n        'Percentage': household_pct\n    })\n})\n\n# Per capita income bands\nhousehold_income['per_capita_band'] = household_income['per_capita_income_mw'].apply(categorize_household_income)\nper_capita_dist = household_income['per_capita_band'].value_counts().sort_index()\nper_capita_pct = (per_capita_dist / per_capita_dist.sum() * 100).round(2)\n\ndisplay({\n    'Per capita income bands': pd.DataFrame({\n        'Count': per_capita_dist,\n        'Percentage': per_capita_pct\n    })\n})",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mblbzz388u",
   "source": "## Income Distribution Visualizations\n\nHistogramas e gr\u00e1ficos da distribui\u00e7\u00e3o de renda.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vkk8fsmp3vh",
   "source": "# Import visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (15, 10)\n\n# Create subplots for income distributions\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# 1. Histogram of individual income in MW\nax1 = axes[0, 0]\ndf_income_plot = df_income[df_income[mw_col] <= 20]  # Limit for better visualization\nax1.hist(df_income_plot[mw_col], bins=50, edgecolor='black', alpha=0.7)\nax1.set_xlabel('Income (Minimum Wages)')\nax1.set_ylabel('Frequency')\nax1.set_title('Individual Income Distribution (MW)')\nax1.axvline(x=1, color='r', linestyle='--', label='1 MW')\nax1.axvline(x=2, color='g', linestyle='--', label='2 MW')\nax1.axvline(x=5, color='b', linestyle='--', label='5 MW')\nax1.legend()\n\n# 2. Histogram of NPV-adjusted income\nax2 = axes[0, 1]\ndf_npv_plot = df_income[df_income[npv_col] <= 30000]  # Limit for better visualization\nax2.hist(df_npv_plot[npv_col], bins=50, edgecolor='black', alpha=0.7, color='orange')\nax2.set_xlabel('Income (R$ - Jul/2025)')\nax2.set_ylabel('Frequency')\nax2.set_title('Individual Income Distribution (NPV-adjusted)')\nax2.axvline(x=MIN_WAGE, color='r', linestyle='--', label=f'1 MW (R$ {MIN_WAGE})')\nax2.axvline(x=MIN_WAGE*2, color='g', linestyle='--', label=f'2 MW')\nax2.axvline(x=MIN_WAGE*5, color='b', linestyle='--', label=f'5 MW')\nax2.legend()\n\n# 3. Bar chart of income bands (individual)\nax3 = axes[0, 2]\nincome_pct_v2.plot(kind='bar', ax=ax3, color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99'])\nax3.set_xlabel('Income Band')\nax3.set_ylabel('Percentage (%)')\nax3.set_title('Individual Income Distribution by Bands')\nax3.set_xticklabels(ax3.get_xticklabels(), rotation=45)\nfor i, v in enumerate(income_pct_v2):\n    ax3.text(i, v + 0.5, f'{v:.1f}%', ha='center')\n\n# 4. Household income histogram\nax4 = axes[1, 0]\nhousehold_plot = household_income[household_income['household_income_mw'] <= 30]\nax4.hist(household_plot['household_income_mw'], bins=50, edgecolor='black', alpha=0.7, color='green')\nax4.set_xlabel('Household Income (MW)')\nax4.set_ylabel('Frequency')\nax4.set_title('Household Income Distribution')\nax4.axvline(x=2, color='r', linestyle='--', label='2 MW')\nax4.axvline(x=5, color='g', linestyle='--', label='5 MW')\nax4.axvline(x=10, color='b', linestyle='--', label='10 MW')\nax4.legend()\n\n# 5. Per capita income histogram\nax5 = axes[1, 1]\nper_capita_plot = household_income[household_income['per_capita_income_mw'] <= 10]\nax5.hist(per_capita_plot['per_capita_income_mw'], bins=50, edgecolor='black', alpha=0.7, color='purple')\nax5.set_xlabel('Per Capita Income (MW)')\nax5.set_ylabel('Frequency')\nax5.set_title('Per Capita Income Distribution')\nax5.axvline(x=0.5, color='r', linestyle='--', label='0.5 MW')\nax5.axvline(x=1, color='g', linestyle='--', label='1 MW')\nax5.axvline(x=2, color='b', linestyle='--', label='2 MW')\nax5.legend()\n\n# 6. Pie chart of household income bands\nax6 = axes[1, 2]\ncolors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\nwedges, texts, autotexts = ax6.pie(household_pct, labels=household_pct.index, colors=colors, \n                                    autopct='%1.1f%%', startangle=90)\nax6.set_title('Household Income Distribution by Bands')\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\ndisplay({\n    'Income Distribution Summary': {\n        'Individual median (MW)': df_income[mw_col].median().round(2),\n        'Individual mean (MW)': df_income[mw_col].mean().round(2),\n        'Household median (MW)': household_income['household_income_mw'].median().round(2),\n        'Household mean (MW)': household_income['household_income_mw'].mean().round(2),\n        'Per capita median (MW)': household_income['per_capita_income_mw'].median().round(2),\n        'Per capita mean (MW)': household_income['per_capita_income_mw'].mean().round(2)\n    }\n})",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xgjza3aujnm",
   "source": "## Geographic and Demographic Analysis\n\nAn\u00e1lise de renda por regi\u00e3o, estado, e caracter\u00edsticas demogr\u00e1ficas.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2yjjvaxdh7i",
   "source": "# Geographic Analysis - Income by State\nstate_income = df_income.groupby('UF_label').agg({\n    mw_col: ['mean', 'median', 'std', 'count']\n}).round(2)\nstate_income.columns = ['Mean MW', 'Median MW', 'Std MW', 'Count']\nstate_income = state_income.sort_values('Median MW', ascending=False)\n\n# Top 10 states by median income\ndisplay({\n    'Top 10 States by Median Income (MW)': state_income.head(10)\n})\n\n# Income by Capital vs Non-Capital\ncapital_income = df_income.groupby('Capital_label').agg({\n    mw_col: ['mean', 'median', 'count']\n}).round(2)\ndisplay({\n    'Income by Capital Status': capital_income\n})\n\n# Demographic Analysis\n# By Sex\nsex_income = df_income.groupby('V2007_label').agg({\n    mw_col: ['mean', 'median', 'count']\n}).round(2)\nsex_income.columns = ['Mean MW', 'Median MW', 'Count']\n\n# By Age Groups\ndf_income['age_group'] = pd.cut(df_income['V2009__idade_na_data_de_referncia'], \n                                bins=[0, 18, 25, 35, 45, 55, 65, 100],\n                                labels=['<18', '18-25', '26-35', '36-45', '46-55', '56-65', '65+'])\nage_income = df_income.groupby('age_group').agg({\n    mw_col: ['mean', 'median', 'count']\n}).round(2)\nage_income.columns = ['Mean MW', 'Median MW', 'Count']\n\n# By Race\nrace_income = df_income.groupby('V2010_label').agg({\n    mw_col: ['mean', 'median', 'count']\n}).round(2)\nrace_income.columns = ['Mean MW', 'Median MW', 'Count']\n\ndisplay({\n    'Income by Sex': sex_income,\n    'Income by Age Group': age_income,\n    'Income by Race': race_income\n})",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8s7q65wu8gh",
   "source": "## Predictive Model for Income Estimation\n\nModelo de machine learning para prever renda baseado em caracter\u00edsticas demogr\u00e1ficas e geogr\u00e1ficas.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "f5rrvaqdljs",
   "source": "# Data Preparation for Predictive Model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Select features for the model\nfeature_columns = [\n    'UF__unidade_da_federao', 'Capital__municpio_da_capital',\n    'V2007__sexo', 'V2009__idade_na_data_de_referncia', 'V2010__cor_ou_raa',\n    'V3001__sabe_ler_e_escrever', 'V3009A__curso_mais_elevado_que_frequentou',\n    'VD2003__nmero_de_componentes_do_domic'\n]\n\n# Prepare dataset\nmodel_df = df_income[feature_columns + [mw_col]].dropna()\n\n# Encode categorical variables\nlabel_encoders = {}\ncategorical_cols = ['UF__unidade_da_federao', 'Capital__municpio_da_capital', \n                   'V2007__sexo', 'V2010__cor_ou_raa', 'V3001__sabe_ler_e_escrever',\n                   'V3009A__curso_mais_elevado_que_frequentou']\n\nfor col in categorical_cols:\n    le = LabelEncoder()\n    model_df[col] = le.fit_transform(model_df[col].astype(str))\n    label_encoders[col] = le\n\n# Split features and target\nX = model_df[feature_columns]\ny = model_df[mw_col]\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\ndisplay({\n    'Dataset shape': model_df.shape,\n    'Training set': X_train.shape,\n    'Test set': X_test.shape,\n    'Target distribution': y.describe().round(2)\n})",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2olv68gzqem",
   "source": "# Model Training and Evaluation\n# Random Forest Model\nrf_model = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1)\nrf_model.fit(X_train, y_train)\nrf_pred = rf_model.predict(X_test)\n\n# Gradient Boosting Model\ngb_model = GradientBoostingRegressor(n_estimators=100, max_depth=10, random_state=42)\ngb_model.fit(X_train, y_train)\ngb_pred = gb_model.predict(X_test)\n\n# Model evaluation\ndef evaluate_model(y_true, y_pred, model_name):\n    return {\n        'Model': model_name,\n        'R\u00b2 Score': round(r2_score(y_true, y_pred), 4),\n        'MAE': round(mean_absolute_error(y_true, y_pred), 3),\n        'RMSE': round(np.sqrt(mean_squared_error(y_true, y_pred)), 3),\n        'Median Abs Error': np.median(np.abs(y_true - y_pred)).round(3)\n    }\n\n# Compare models\nresults = pd.DataFrame([\n    evaluate_model(y_test, rf_pred, 'Random Forest'),\n    evaluate_model(y_test, gb_pred, 'Gradient Boosting')\n])\n\ndisplay({\n    'Model Performance': results\n})\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    'Feature': feature_columns,\n    'RF_Importance': rf_model.feature_importances_,\n    'GB_Importance': gb_model.feature_importances_\n}).sort_values('RF_Importance', ascending=False)\n\ndisplay({\n    'Feature Importance': feature_importance.round(4)\n})\n\n# Visualize predictions vs actual\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Random Forest\nax1 = axes[0]\nax1.scatter(y_test, rf_pred, alpha=0.5, s=10)\nax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nax1.set_xlabel('Actual Income (MW)')\nax1.set_ylabel('Predicted Income (MW)')\nax1.set_title(f'Random Forest (R\u00b2={r2_score(y_test, rf_pred):.3f})')\nax1.set_xlim(0, 15)\nax1.set_ylim(0, 15)\n\n# Gradient Boosting\nax2 = axes[1]\nax2.scatter(y_test, gb_pred, alpha=0.5, s=10, color='green')\nax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nax2.set_xlabel('Actual Income (MW)')\nax2.set_ylabel('Predicted Income (MW)')\nax2.set_title(f'Gradient Boosting (R\u00b2={r2_score(y_test, gb_pred):.3f})')\nax2.set_xlim(0, 15)\nax2.set_ylim(0, 15)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ikkr5plp4q",
   "source": "# Advanced Statistical Analysis\n\nEste se\u00e7\u00e3o cont\u00e9m an\u00e1lises estat\u00edsticas avan\u00e7adas para desigualdade de renda, testes estat\u00edsticos, mobilidade de renda e an\u00e1lises multivariadas.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "943yw4z99uw",
   "source": "# Import additional libraries for advanced analysis\nfrom scipy import stats\nfrom scipy.stats import mannwhitneyu, kruskal, chi2_contingency, ttest_ind\nfrom scipy.optimize import minimize_scalar\nimport statsmodels.api as sm\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Advanced statistical analysis libraries loaded successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bqrfruho7i",
   "source": "## 1. Gini Coefficient and Lorenz Curve Analysis\n\nC\u00e1lculo do coeficiente de Gini e constru\u00e7\u00e3o da curva de Lorenz para an\u00e1lise da desigualdade de renda.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "h9ns4n3pv8",
   "source": "def calculate_gini(income_array):\n    \"\"\"\n    Calculate Gini coefficient for income distribution.\n    \n    Args:\n        income_array: Array of income values\n    \n    Returns:\n        Gini coefficient (0 = perfect equality, 1 = perfect inequality)\n    \"\"\"\n    # Remove NaN and negative values\n    income = np.array(income_array)\n    income = income[~np.isnan(income)]\n    income = income[income >= 0]\n    \n    if len(income) == 0:\n        return np.nan\n    \n    # Sort income array\n    income_sorted = np.sort(income)\n    n = len(income_sorted)\n    \n    # Calculate Gini coefficient using the formula:\n    # G = (2 * sum(i * y_i)) / (n * sum(y_i)) - (n + 1) / n\n    cumsum = np.cumsum(income_sorted)\n    gini = (2 * np.sum((np.arange(1, n + 1) * income_sorted))) / (n * np.sum(income_sorted)) - (n + 1) / n\n    \n    return gini\n\ndef calculate_lorenz_curve(income_array):\n    \"\"\"\n    Calculate Lorenz curve coordinates.\n    \n    Args:\n        income_array: Array of income values\n    \n    Returns:\n        tuple: (population_cumulative, income_cumulative) for plotting\n    \"\"\"\n    # Remove NaN and negative values\n    income = np.array(income_array)\n    income = income[~np.isnan(income)]\n    income = income[income >= 0]\n    \n    if len(income) == 0:\n        return np.array([]), np.array([])\n    \n    # Sort income\n    income_sorted = np.sort(income)\n    n = len(income_sorted)\n    \n    # Calculate cumulative proportions\n    population_cumulative = np.arange(1, n + 1) / n\n    income_cumulative = np.cumsum(income_sorted) / np.sum(income_sorted)\n    \n    # Add origin point\n    population_cumulative = np.concatenate([[0], population_cumulative])\n    income_cumulative = np.concatenate([[0], income_cumulative])\n    \n    return population_cumulative, income_cumulative\n\n# Calculate Gini coefficient for different income measures\ngini_results = {}\n\n# Individual income (MW)\ngini_individual_mw = calculate_gini(df_income[mw_col])\ngini_results['Individual Income (MW)'] = gini_individual_mw\n\n# Individual income (NPV-adjusted)\ngini_individual_npv = calculate_gini(df_income[npv_col])\ngini_results['Individual Income (NPV)'] = gini_individual_npv\n\n# Household income (MW)\ngini_household_mw = calculate_gini(household_income['household_income_mw'])\ngini_results['Household Income (MW)'] = gini_household_mw\n\n# Per capita income (MW)\ngini_per_capita_mw = calculate_gini(household_income['per_capita_income_mw'])\ngini_results['Per Capita Income (MW)'] = gini_per_capita_mw\n\n# Display Gini coefficients\ngini_df = pd.DataFrame(list(gini_results.items()), columns=['Income Type', 'Gini Coefficient'])\ngini_df['Gini Coefficient'] = gini_df['Gini Coefficient'].round(4)\n\ndisplay({\n    'Gini Coefficients': gini_df\n})\n\nprint(\"\\nInterpretation:\")\nprint(\"- Gini = 0: Perfect equality (everyone has same income)\")\nprint(\"- Gini = 1: Perfect inequality (one person has all income)\")\nprint(\"- Values 0.25-0.35: Relatively equal\")\nprint(\"- Values 0.35-0.45: Moderately unequal\")\nprint(\"- Values > 0.45: Highly unequal\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1dro1r1ex6w",
   "source": "# Plot Lorenz Curves\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Individual Income (MW) Lorenz Curve\nax1 = axes[0, 0]\npop_cum, inc_cum = calculate_lorenz_curve(df_income[mw_col])\nax1.plot(pop_cum, inc_cum, 'b-', linewidth=2, label='Lorenz Curve')\nax1.plot([0, 1], [0, 1], 'r--', linewidth=1, label='Line of Equality')\nax1.fill_between(pop_cum, inc_cum, 0, alpha=0.3, color='lightblue')\nax1.set_xlabel('Cumulative Share of Population')\nax1.set_ylabel('Cumulative Share of Income')\nax1.set_title(f'Individual Income Lorenz Curve\\n(Gini = {gini_individual_mw:.4f})')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Household Income Lorenz Curve\nax2 = axes[0, 1]\npop_cum_h, inc_cum_h = calculate_lorenz_curve(household_income['household_income_mw'])\nax2.plot(pop_cum_h, inc_cum_h, 'g-', linewidth=2, label='Lorenz Curve')\nax2.plot([0, 1], [0, 1], 'r--', linewidth=1, label='Line of Equality')\nax2.fill_between(pop_cum_h, inc_cum_h, 0, alpha=0.3, color='lightgreen')\nax2.set_xlabel('Cumulative Share of Households')\nax2.set_ylabel('Cumulative Share of Household Income')\nax2.set_title(f'Household Income Lorenz Curve\\n(Gini = {gini_household_mw:.4f})')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Per Capita Income Lorenz Curve\nax3 = axes[1, 0]\npop_cum_pc, inc_cum_pc = calculate_lorenz_curve(household_income['per_capita_income_mw'])\nax3.plot(pop_cum_pc, inc_cum_pc, 'm-', linewidth=2, label='Lorenz Curve')\nax3.plot([0, 1], [0, 1], 'r--', linewidth=1, label='Line of Equality')\nax3.fill_between(pop_cum_pc, inc_cum_pc, 0, alpha=0.3, color='lavender')\nax3.set_xlabel('Cumulative Share of Population')\nax3.set_ylabel('Cumulative Share of Per Capita Income')\nax3.set_title(f'Per Capita Income Lorenz Curve\\n(Gini = {gini_per_capita_mw:.4f})')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Comparison of all Lorenz curves\nax4 = axes[1, 1]\nax4.plot(pop_cum, inc_cum, 'b-', linewidth=2, label=f'Individual (Gini={gini_individual_mw:.3f})')\nax4.plot(pop_cum_h, inc_cum_h, 'g-', linewidth=2, label=f'Household (Gini={gini_household_mw:.3f})')\nax4.plot(pop_cum_pc, inc_cum_pc, 'm-', linewidth=2, label=f'Per Capita (Gini={gini_per_capita_mw:.3f})')\nax4.plot([0, 1], [0, 1], 'r--', linewidth=1, label='Perfect Equality')\nax4.set_xlabel('Cumulative Share of Population')\nax4.set_ylabel('Cumulative Share of Income')\nax4.set_title('Comparison of Lorenz Curves')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Calculate income shares by deciles\ndef calculate_decile_shares(income_array):\n    \"\"\"Calculate income share for each decile.\"\"\"\n    income = np.array(income_array)\n    income = income[~np.isnan(income)]\n    income = income[income >= 0]\n    \n    if len(income) == 0:\n        return np.array([])\n    \n    # Calculate deciles\n    deciles = np.percentile(income, np.arange(10, 101, 10))\n    decile_labels = [f'D{i}' for i in range(1, 11)]\n    \n    # Assign each person to a decile\n    decile_assignment = np.digitize(income, deciles)\n    \n    # Calculate total income for each decile\n    total_income = np.sum(income)\n    decile_shares = []\n    \n    for i in range(1, 11):\n        decile_income = np.sum(income[decile_assignment == i-1])\n        share = decile_income / total_income * 100\n        decile_shares.append(share)\n    \n    return np.array(decile_shares), decile_labels\n\n# Calculate and display decile shares\ndecile_shares, decile_labels = calculate_decile_shares(df_income[mw_col])\n\ndecile_df = pd.DataFrame({\n    'Decile': decile_labels,\n    'Income Share (%)': decile_shares.round(2)\n})\n\ndisplay({\n    'Income Distribution by Deciles': decile_df,\n    'Bottom 50% share': f\"{decile_shares[:5].sum():.1f}%\",\n    'Top 10% share': f\"{decile_shares[-1]:.1f}%\",\n    'Top 1% analysis': \"Use percentile 99 for more detailed top income analysis\"\n})\n\n# Calculate Palma ratio (top 10% / bottom 40%)\npalma_ratio = decile_shares[-1] / decile_shares[:4].sum()\ndisplay({\n    'Palma Ratio': f\"{palma_ratio:.2f} (Top 10% vs Bottom 40%)\"\n})",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5tcwulm3kqw",
   "source": "## 2. Statistical Tests for Income Distribution Comparisons\n\nTestes estat\u00edsticos para comparar distribui\u00e7\u00f5es de renda entre diferentes grupos demogr\u00e1ficos.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4nhpz6dxsex",
   "source": "def perform_income_tests(df, income_col, group_col, group_name):\n    \"\"\"\n    Perform comprehensive statistical tests comparing income distributions across groups.\n    \n    Args:\n        df: DataFrame with income and grouping data\n        income_col: Name of income column\n        group_col: Name of grouping column\n        group_name: Descriptive name for the grouping variable\n    \n    Returns:\n        Dictionary with test results\n    \"\"\"\n    results = {'group_name': group_name}\n    \n    # Clean data\n    df_clean = df[[income_col, group_col]].dropna()\n    df_clean = df_clean[df_clean[income_col] > 0]\n    \n    # Get unique groups\n    groups = df_clean[group_col].unique()\n    results['groups'] = groups\n    results['group_sizes'] = df_clean[group_col].value_counts().to_dict()\n    \n    # Descriptive statistics by group\n    desc_stats = df_clean.groupby(group_col)[income_col].agg([\n        'count', 'mean', 'median', 'std', 'min', 'max'\n    ]).round(3)\n    results['descriptive_stats'] = desc_stats\n    \n    # 1. Kruskal-Wallis Test (non-parametric ANOVA)\n    group_data = [df_clean[df_clean[group_col] == group][income_col].values for group in groups]\n    kw_stat, kw_p = kruskal(*group_data)\n    results['kruskal_wallis'] = {\n        'statistic': kw_stat,\n        'p_value': kw_p,\n        'significant': kw_p < 0.05,\n        'interpretation': 'Significant differences between groups' if kw_p < 0.05 else 'No significant differences'\n    }\n    \n    # 2. Pairwise Mann-Whitney U tests (if more than 2 groups)\n    if len(groups) > 2:\n        pairwise_results = {}\n        for i, group1 in enumerate(groups):\n            for j, group2 in enumerate(groups):\n                if i < j:  # Avoid duplicate comparisons\n                    data1 = df_clean[df_clean[group_col] == group1][income_col]\n                    data2 = df_clean[df_clean[group_col] == group2][income_col]\n                    \n                    stat, p_val = mannwhitneyu(data1, data2, alternative='two-sided')\n                    pairwise_results[f'{group1}_vs_{group2}'] = {\n                        'statistic': stat,\n                        'p_value': p_val,\n                        'significant': p_val < 0.05\n                    }\n        results['pairwise_mann_whitney'] = pairwise_results\n    \n    # 3. Effect size (Cohen's d for each pair vs overall mean)\n    overall_mean = df_clean[income_col].mean()\n    overall_std = df_clean[income_col].std()\n    \n    effect_sizes = {}\n    for group in groups:\n        group_mean = df_clean[df_clean[group_col] == group][income_col].mean()\n        cohens_d = (group_mean - overall_mean) / overall_std\n        effect_sizes[group] = cohens_d\n    \n    results['effect_sizes'] = effect_sizes\n    \n    # 4. Levene's test for homogeneity of variance\n    levene_stat, levene_p = stats.levene(*group_data)\n    results['levene_test'] = {\n        'statistic': levene_stat,\n        'p_value': levene_p,\n        'equal_variances': levene_p > 0.05\n    }\n    \n    return results\n\n# Test 1: Income by Gender\nprint(\"=== STATISTICAL TESTS FOR INCOME DISTRIBUTION COMPARISONS ===\\n\")\n\ngender_tests = perform_income_tests(df_income, mw_col, 'V2007_label', 'Gender')\n\nprint(\"1. INCOME BY GENDER\")\nprint(\"-\" * 50)\nprint(f\"Groups: {gender_tests['groups']}\")\nprint(f\"Sample sizes: {gender_tests['group_sizes']}\")\nprint(\"\\nDescriptive Statistics:\")\ndisplay(gender_tests['descriptive_stats'])\n\nprint(f\"\\nKruskal-Wallis Test:\")\nprint(f\"  Statistic: {gender_tests['kruskal_wallis']['statistic']:.4f}\")\nprint(f\"  P-value: {gender_tests['kruskal_wallis']['p_value']:.6f}\")\nprint(f\"  Result: {gender_tests['kruskal_wallis']['interpretation']}\")\n\nif 'pairwise_mann_whitney' in gender_tests:\n    print(f\"\\nPairwise Mann-Whitney U Tests:\")\n    for comparison, result in gender_tests['pairwise_mann_whitney'].items():\n        print(f\"  {comparison}: p = {result['p_value']:.6f} {'*' if result['significant'] else ''}\")\n\nprint(f\"\\nEffect Sizes (Cohen's d):\")\nfor group, effect in gender_tests['effect_sizes'].items():\n    magnitude = 'small' if abs(effect) < 0.5 else 'medium' if abs(effect) < 0.8 else 'large'\n    print(f\"  {group}: {effect:.3f} ({magnitude})\")\n\nprint(f\"\\nLevene's Test for Equal Variances:\")\nprint(f\"  P-value: {gender_tests['levene_test']['p_value']:.6f}\")\nprint(f\"  Equal variances: {gender_tests['levene_test']['equal_variances']}\")\n\nprint(\"\\n\" + \"=\"*70 + \"\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "med32uav8d9",
   "source": "# Test 2: Income by Race\nrace_tests = perform_income_tests(df_income, mw_col, 'V2010_label', 'Race')\n\nprint(\"2. INCOME BY RACE\")\nprint(\"-\" * 50)\nprint(f\"Groups: {race_tests['groups']}\")\nprint(f\"Sample sizes: {race_tests['group_sizes']}\")\nprint(\"\\nDescriptive Statistics:\")\ndisplay(race_tests['descriptive_stats'])\n\nprint(f\"\\nKruskal-Wallis Test:\")\nprint(f\"  Statistic: {race_tests['kruskal_wallis']['statistic']:.4f}\")\nprint(f\"  P-value: {race_tests['kruskal_wallis']['p_value']:.6f}\")\nprint(f\"  Result: {race_tests['kruskal_wallis']['interpretation']}\")\n\nif 'pairwise_mann_whitney' in race_tests:\n    print(f\"\\nPairwise Mann-Whitney U Tests:\")\n    for comparison, result in race_tests['pairwise_mann_whitney'].items():\n        print(f\"  {comparison}: p = {result['p_value']:.6f} {'*' if result['significant'] else ''}\")\n\nprint(f\"\\nEffect Sizes (Cohen's d):\")\nfor group, effect in race_tests['effect_sizes'].items():\n    magnitude = 'small' if abs(effect) < 0.5 else 'medium' if abs(effect) < 0.8 else 'large'\n    print(f\"  {group}: {effect:.3f} ({magnitude})\")\n\nprint(\"\\n\" + \"=\"*70 + \"\\n\")\n\n# Test 3: Income by Age Group\nage_tests = perform_income_tests(df_income, mw_col, 'age_group', 'Age Group')\n\nprint(\"3. INCOME BY AGE GROUP\")\nprint(\"-\" * 50)\nprint(f\"Groups: {age_tests['groups']}\")\nprint(f\"Sample sizes: {age_tests['group_sizes']}\")\nprint(\"\\nDescriptive Statistics:\")\ndisplay(age_tests['descriptive_stats'])\n\nprint(f\"\\nKruskal-Wallis Test:\")\nprint(f\"  Statistic: {age_tests['kruskal_wallis']['statistic']:.4f}\")\nprint(f\"  P-value: {age_tests['kruskal_wallis']['p_value']:.6f}\")\nprint(f\"  Result: {age_tests['kruskal_wallis']['interpretation']}\")\n\nprint(f\"\\nEffect Sizes (Cohen's d):\")\nfor group, effect in age_tests['effect_sizes'].items():\n    if pd.notna(effect):\n        magnitude = 'small' if abs(effect) < 0.5 else 'medium' if abs(effect) < 0.8 else 'large'\n        print(f\"  {group}: {effect:.3f} ({magnitude})\")\n\nprint(\"\\n\" + \"=\"*70 + \"\\n\")\n\n# Test 4: Income by Region (using top 10 states by sample size)\ntop_states = df_income['UF_label'].value_counts().head(10).index\ndf_income_states = df_income[df_income['UF_label'].isin(top_states)]\n\nstate_tests = perform_income_tests(df_income_states, mw_col, 'UF_label', 'State (Top 10)')\n\nprint(\"4. INCOME BY STATE (TOP 10 BY SAMPLE SIZE)\")\nprint(\"-\" * 50)\nprint(f\"States analyzed: {list(state_tests['groups'])}\")\nprint(f\"Sample sizes: {state_tests['group_sizes']}\")\n\nprint(f\"\\nKruskal-Wallis Test:\")\nprint(f\"  Statistic: {state_tests['kruskal_wallis']['statistic']:.4f}\")\nprint(f\"  P-value: {state_tests['kruskal_wallis']['p_value']:.6f}\")\nprint(f\"  Result: {state_tests['kruskal_wallis']['interpretation']}\")\n\n# Show descriptive stats for top 5 and bottom 5 states by median income\nstate_medians = state_tests['descriptive_stats']['median'].sort_values(ascending=False)\nprint(f\"\\nTop 5 States by Median Income (MW):\")\nfor state, median in state_medians.head(5).items():\n    print(f\"  {state}: {median:.3f}\")\n    \nprint(f\"\\nBottom 5 States by Median Income (MW):\")\nfor state, median in state_medians.tail(5).items():\n    print(f\"  {state}: {median:.3f}\")\n\nprint(\"\\n\" + \"=\"*70 + \"\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6bksjdmi5if",
   "source": "## 3. Income Mobility Analysis\n\nAn\u00e1lise de mobilidade de renda simulada atrav\u00e9s de transi\u00e7\u00f5es entre faixas de renda ao longo do tempo (usando dados de trimestres diferentes como proxy).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vgsz4bzhec8",
   "source": "# Income Mobility Analysis\n# Note: This is a cross-sectional simulation since PNADC doesn't have panel data\n# We'll use different quarters and age cohorts as a proxy for longitudinal analysis\n\ndef create_income_brackets(income_series, n_brackets=5):\n    \"\"\"Create income brackets based on percentiles.\"\"\"\n    percentiles = np.linspace(0, 100, n_brackets + 1)\n    thresholds = np.percentile(income_series.dropna(), percentiles)\n    \n    def assign_bracket(income):\n        if pd.isna(income) or income <= 0:\n            return np.nan\n        for i, threshold in enumerate(thresholds[1:], 1):\n            if income <= threshold:\n                return f'Q{i}'\n        return f'Q{n_brackets}'\n    \n    return income_series.apply(assign_bracket), thresholds\n\n# Create mobility analysis across quarters\nprint(\"=== INCOME MOBILITY ANALYSIS ===\\n\")\n\n# 1. Create income quintiles for the entire dataset\ndf_mobility = df[df[mw_col].notna() & (df[mw_col] > 0)].copy()\ndf_mobility['income_quintile'], quintile_thresholds = create_income_brackets(df_mobility[mw_col], 5)\n\nprint(\"1. INCOME QUINTILE DEFINITIONS (MW)\")\nprint(\"-\" * 50)\nfor i, threshold in enumerate(quintile_thresholds[1:], 1):\n    lower = quintile_thresholds[i-1] if i > 1 else 0\n    print(f\"Q{i}: {lower:.2f} - {threshold:.2f} MW\")\n\n# Distribution across quintiles\nquintile_dist = df_mobility['income_quintile'].value_counts().sort_index()\nprint(f\"\\nDistribution across quintiles:\")\nfor q, count in quintile_dist.items():\n    pct = count / len(df_mobility) * 100\n    print(f\"{q}: {count:,} ({pct:.1f}%)\")\n\nprint(\"\\n\" + \"=\"*70 + \"\\n\")\n\n# 2. Simulate mobility across quarters (cross-sectional approximation)\n# Group by UF and compare income distribution patterns across quarters\n\nmobility_matrix = pd.DataFrame(index=[f'Q{i}' for i in range(1, 6)], \n                              columns=[f'Q{i}' for i in range(1, 6)], \n                              data=0.0)\n\n# Create age-education cohorts for pseudo-panel analysis\ndf_mobility['age_cohort'] = pd.cut(df_mobility['V2009__idade_na_data_de_referncia'], \n                                   bins=[0, 25, 35, 45, 55, 65, 100],\n                                   labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n\ndf_mobility['education_level'] = df_mobility['V3009A__curso_mais_elevado_que_frequentou'].astype(str)\n\n# Calculate transition probabilities by comparing similar cohorts across quarters\ntransitions = []\n\nfor uf in df_mobility['UF_label'].unique():\n    uf_data = df_mobility[df_mobility['UF_label'] == uf]\n    \n    for cohort in df_mobility['age_cohort'].unique():\n        if pd.isna(cohort):\n            continue\n            \n        cohort_data = uf_data[uf_data['age_cohort'] == cohort]\n        \n        if len(cohort_data) < 10:  # Skip small groups\n            continue\n            \n        # Compare quintile distributions across available quarters\n        quarters = cohort_data['Trimestre__trimestre_de_referncia'].unique()\n        \n        if len(quarters) > 1:\n            # Calculate mobility between first and last available quarter\n            q1_data = cohort_data[cohort_data['Trimestre__trimestre_de_referncia'] == quarters[0]]\n            q2_data = cohort_data[cohort_data['Trimestre__trimestre_de_referncia'] == quarters[-1]]\n            \n            if len(q1_data) > 5 and len(q2_data) > 5:\n                # Calculate quintile distributions\n                q1_dist = q1_data['income_quintile'].value_counts(normalize=True)\n                q2_dist = q2_data['income_quintile'].value_counts(normalize=True)\n                \n                transitions.append({\n                    'uf': uf,\n                    'cohort': cohort,\n                    'q1_dist': q1_dist,\n                    'q2_dist': q2_dist,\n                    'sample_size': len(q1_data) + len(q2_data)\n                })\n\nprint(\"2. PSEUDO-MOBILITY ANALYSIS\")\nprint(\"-\" * 50)\nprint(f\"Analyzed {len(transitions)} UF-cohort combinations\")\n\n# Calculate average mobility patterns\nif transitions:\n    # Aggregate transition patterns\n    avg_initial = pd.Series([0.0] * 5, index=[f'Q{i}' for i in range(1, 6)])\n    avg_final = pd.Series([0.0] * 5, index=[f'Q{i}' for i in range(1, 6)])\n    \n    for t in transitions:\n        weight = t['sample_size'] / sum([x['sample_size'] for x in transitions])\n        \n        for quintile in [f'Q{i}' for i in range(1, 6)]:\n            avg_initial[quintile] += t['q1_dist'].get(quintile, 0) * weight\n            avg_final[quintile] += t['q2_dist'].get(quintile, 0) * weight\n    \n    mobility_df = pd.DataFrame({\n        'Initial Distribution': avg_initial * 100,\n        'Final Distribution': avg_final * 100,\n        'Change (pp)': (avg_final - avg_initial) * 100\n    }).round(2)\n    \n    print(\"\\nAggregated Mobility Patterns (Percentage Points):\")\n    display(mobility_df)\n    \n    # Calculate mobility indices\n    upward_mobility = sum([max(0, change) for change in mobility_df['Change (pp)'].values])\n    downward_mobility = sum([abs(min(0, change)) for change in mobility_df['Change (pp)'].values])\n    net_mobility = upward_mobility - downward_mobility\n    \n    print(f\"\\nMobility Indices:\")\n    print(f\"  Upward mobility: {upward_mobility:.2f} pp\")\n    print(f\"  Downward mobility: {downward_mobility:.2f} pp\")\n    print(f\"  Net mobility: {net_mobility:.2f} pp\")\n    \n    # Persistence rates (percentage staying in same quintile)\n    persistence = 100 - (upward_mobility + downward_mobility)\n    print(f\"  Income persistence: {persistence:.2f}%\")\n\nprint(\"\\n\" + \"=\"*70 + \"\\n\")\n\n# 3. Intergenerational mobility proxy using age and education\nprint(\"3. INTERGENERATIONAL MOBILITY ANALYSIS (CROSS-SECTIONAL)\")\nprint(\"-\" * 50)\n\n# Use age and household composition as proxy for generations\n# Use V2005_label for text values, or V2005__condio_no_domiclio == 1 for household head\ndf_mobility['is_household_head'] = df_mobility['V2005__condio_no_domiclio'] == 1  # Code 1 = Pessoa respons\u00e1vel\n\n# Compare income by age groups within households\nhousehold_income_analysis = []\n\nfor dom_id in df_mobility['dom_id'].unique():\n    household = df_mobility[df_mobility['dom_id'] == dom_id]\n    \n    if len(household) >= 2:  # Multi-person households\n        # Get household head and others\n        head = household[household['is_household_head'] == True]\n        others = household[household['is_household_head'] == False]\n        \n        if len(head) > 0 and len(others) > 0:\n            head_income = head[mw_col].mean()\n            others_income = others[mw_col].mean()\n            head_age = head['V2009__idade_na_data_de_referncia'].mean()\n            others_age = others['V2009__idade_na_data_de_referncia'].mean()\n            \n            if pd.notna(head_income) and pd.notna(others_income) and head_income > 0 and others_income > 0:\n                household_income_analysis.append({\n                    'dom_id': dom_id,\n                    'head_income': head_income,\n                    'others_income': others_income,\n                    'head_age': head_age,\n                    'others_age': others_age,\n                    'income_ratio': others_income / head_income,\n                    'age_diff': head_age - others_age\n                })\n\nintergenerational_df = pd.DataFrame(household_income_analysis)\n\nif len(intergenerational_df) > 0:\n    print(f\"Analyzed {len(intergenerational_df)} multi-person households\")\n    \n    # Calculate correlation between head and others income\n    income_correlation = intergenerational_df['head_income'].corr(intergenerational_df['others_income'])\n    print(f\"Income correlation (head vs others): {income_correlation:.4f}\")\n    \n    # Income mobility by age difference\n    intergenerational_df['age_gap_group'] = pd.cut(intergenerational_df['age_diff'], \n                                                  bins=[0, 15, 25, 35, 100],\n                                                  labels=['0-15y', '15-25y', '25-35y', '35+y'])\n    \n    mobility_by_age_gap = intergenerational_df.groupby('age_gap_group').agg({\n        'income_ratio': ['mean', 'median', 'std', 'count']\n    }).round(3)\n    \n    print(\"\\nIncome Ratio (Others/Head) by Age Gap:\")\n    display(mobility_by_age_gap)\n    \n    # Rank mobility analysis\n    intergenerational_df['head_quintile'] = pd.qcut(intergenerational_df['head_income'], 5, labels=[f'Q{i}' for i in range(1, 6)])\n    intergenerational_df['others_quintile'] = pd.qcut(intergenerational_df['others_income'], 5, labels=[f'Q{i}' for i in range(1, 6)])\n    \n    # Transition matrix\n    transition_matrix = pd.crosstab(intergenerational_df['head_quintile'], \n                                   intergenerational_df['others_quintile'], \n                                   normalize='index') * 100\n    \n    print(\"\\nIntergenerational Transition Matrix (Row %):\")\n    print(\"(Head Quintile \u2192 Others Quintile)\")\n    display(transition_matrix.round(1))\n    \n    # Calculate intergenerational mobility index\n    diagonal_sum = sum([transition_matrix.loc[f'Q{i}', f'Q{i}'] for i in range(1, 6)])\n    mobility_index = 100 - (diagonal_sum / 5)\n    print(f\"\\nIntergenerational Mobility Index: {mobility_index:.2f}%\")\n    print(\"(Higher values indicate more mobility between generations)\")\n\nprint(\"\\n\" + \"=\"*70 + \"\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7begmnkqne6",
   "source": "## 4. Confidence Intervals for Mean Income by State\n\nC\u00e1lculo de intervalos de confian\u00e7a para renda m\u00e9dia por estado usando bootstrap e m\u00e9todos param\u00e9tricos.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pob2j1wptrl",
   "source": "def bootstrap_confidence_interval(data, n_bootstrap=1000, confidence_level=0.95, statistic='mean'):\n    \"\"\"\n    Calculate bootstrap confidence interval for a statistic.\n    \n    Args:\n        data: Array of data points\n        n_bootstrap: Number of bootstrap samples\n        confidence_level: Confidence level (default 0.95)\n        statistic: Statistic to calculate ('mean' or 'median')\n    \n    Returns:\n        Dictionary with point estimate and confidence interval\n    \"\"\"\n    data = np.array(data)\n    data = data[~np.isnan(data)]\n    \n    if len(data) == 0:\n        return {'point_estimate': np.nan, 'lower_ci': np.nan, 'upper_ci': np.nan}\n    \n    # Original statistic\n    if statistic == 'mean':\n        point_estimate = np.mean(data)\n    else:\n        point_estimate = np.median(data)\n    \n    # Bootstrap samples\n    bootstrap_stats = []\n    n = len(data)\n    \n    for _ in range(n_bootstrap):\n        bootstrap_sample = np.random.choice(data, size=n, replace=True)\n        if statistic == 'mean':\n            bootstrap_stats.append(np.mean(bootstrap_sample))\n        else:\n            bootstrap_stats.append(np.median(bootstrap_sample))\n    \n    # Calculate confidence intervals\n    alpha = 1 - confidence_level\n    lower_percentile = (alpha / 2) * 100\n    upper_percentile = (1 - alpha / 2) * 100\n    \n    lower_ci = np.percentile(bootstrap_stats, lower_percentile)\n    upper_ci = np.percentile(bootstrap_stats, upper_percentile)\n    \n    return {\n        'point_estimate': point_estimate,\n        'lower_ci': lower_ci,\n        'upper_ci': upper_ci,\n        'ci_width': upper_ci - lower_ci,\n        'margin_error': (upper_ci - lower_ci) / 2\n    }\n\ndef parametric_confidence_interval(data, confidence_level=0.95):\n    \"\"\"\n    Calculate parametric confidence interval assuming normal distribution.\n    \n    Args:\n        data: Array of data points\n        confidence_level: Confidence level (default 0.95)\n    \n    Returns:\n        Dictionary with confidence interval\n    \"\"\"\n    data = np.array(data)\n    data = data[~np.isnan(data)]\n    \n    if len(data) <= 1:\n        return {'point_estimate': np.nan, 'lower_ci': np.nan, 'upper_ci': np.nan}\n    \n    mean = np.mean(data)\n    std_error = stats.sem(data)  # Standard error of the mean\n    \n    # t-distribution for small samples\n    df = len(data) - 1\n    alpha = 1 - confidence_level\n    t_critical = stats.t.ppf(1 - alpha / 2, df)\n    \n    margin_error = t_critical * std_error\n    \n    return {\n        'point_estimate': mean,\n        'lower_ci': mean - margin_error,\n        'upper_ci': mean + margin_error,\n        'ci_width': 2 * margin_error,\n        'margin_error': margin_error,\n        'std_error': std_error,\n        't_critical': t_critical\n    }\n\n# Calculate confidence intervals for mean income by state\nprint(\"=== CONFIDENCE INTERVALS FOR MEAN INCOME BY STATE ===\\n\")\n\n# Filter states with sufficient sample size\nstate_sample_sizes = df_income['UF_label'].value_counts()\nstates_with_sufficient_data = state_sample_sizes[state_sample_sizes >= 100].index\n\nprint(f\"Analyzing {len(states_with_sufficient_data)} states with \u2265100 observations\")\nprint(f\"Total observations: {state_sample_sizes[states_with_sufficient_data].sum():,}\")\n\n# Calculate confidence intervals for each state\nconfidence_results = []\n\nfor state in states_with_sufficient_data:\n    state_income = df_income[df_income['UF_label'] == state][mw_col]\n    \n    # Bootstrap CI (non-parametric)\n    bootstrap_mean = bootstrap_confidence_interval(state_income, statistic='mean')\n    bootstrap_median = bootstrap_confidence_interval(state_income, statistic='median')\n    \n    # Parametric CI (assuming normality)\n    parametric_mean = parametric_confidence_interval(state_income)\n    \n    confidence_results.append({\n        'State': state,\n        'Sample_Size': len(state_income),\n        'Mean': bootstrap_mean['point_estimate'],\n        'Bootstrap_Mean_Lower': bootstrap_mean['lower_ci'],\n        'Bootstrap_Mean_Upper': bootstrap_mean['upper_ci'],\n        'Bootstrap_Mean_Width': bootstrap_mean['ci_width'],\n        'Parametric_Mean_Lower': parametric_mean['lower_ci'],\n        'Parametric_Mean_Upper': parametric_mean['upper_ci'],\n        'Parametric_Mean_Width': parametric_mean['ci_width'],\n        'Median': bootstrap_median['point_estimate'],\n        'Bootstrap_Median_Lower': bootstrap_median['lower_ci'],\n        'Bootstrap_Median_Upper': bootstrap_median['upper_ci'],\n        'Standard_Error': parametric_mean['std_error']\n    })\n\n# Create results DataFrame\nconfidence_df = pd.DataFrame(confidence_results).round(4)\nconfidence_df = confidence_df.sort_values('Mean', ascending=False)\n\n# Display top 10 states by mean income with confidence intervals\nprint(\"1. TOP 10 STATES BY MEAN INCOME (with 95% Confidence Intervals)\")\nprint(\"-\" * 80)\n\ntop10_df = confidence_df.head(10)[['State', 'Sample_Size', 'Mean', \n                                   'Bootstrap_Mean_Lower', 'Bootstrap_Mean_Upper', \n                                   'Bootstrap_Mean_Width']]\n\nfor _, row in top10_df.iterrows():\n    print(f\"{row['State']:25} | n={row['Sample_Size']:5} | \"\n          f\"Mean: {row['Mean']:.3f} MW \"\n          f\"[{row['Bootstrap_Mean_Lower']:.3f}, {row['Bootstrap_Mean_Upper']:.3f}] \"\n          f\"(\u00b1{row['Bootstrap_Mean_Width']/2:.3f})\")\n\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\n# Display bottom 10 states\nprint(\"2. BOTTOM 10 STATES BY MEAN INCOME (with 95% Confidence Intervals)\")\nprint(\"-\" * 80)\n\nbottom10_df = confidence_df.tail(10)[['State', 'Sample_Size', 'Mean', \n                                      'Bootstrap_Mean_Lower', 'Bootstrap_Mean_Upper', \n                                      'Bootstrap_Mean_Width']]\n\nfor _, row in bottom10_df.iterrows():\n    print(f\"{row['State']:25} | n={row['Sample_Size']:5} | \"\n          f\"Mean: {row['Mean']:.3f} MW \"\n          f\"[{row['Bootstrap_Mean_Lower']:.3f}, {row['Bootstrap_Mean_Upper']:.3f}] \"\n          f\"(\u00b1{row['Bootstrap_Mean_Width']/2:.3f})\")\n\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\n# Compare bootstrap vs parametric confidence intervals\nprint(\"3. BOOTSTRAP vs PARAMETRIC CONFIDENCE INTERVALS COMPARISON\")\nprint(\"-\" * 80)\n\ncomparison_df = confidence_df[['State', 'Mean', 'Bootstrap_Mean_Width', 'Parametric_Mean_Width']].copy()\ncomparison_df['Width_Difference'] = comparison_df['Bootstrap_Mean_Width'] - comparison_df['Parametric_Mean_Width']\ncomparison_df['Width_Ratio'] = comparison_df['Bootstrap_Mean_Width'] / comparison_df['Parametric_Mean_Width']\n\n# Show states with largest differences\nprint(\"States with largest differences in CI width (Bootstrap vs Parametric):\")\nextreme_diff = comparison_df.nlargest(5, 'Width_Difference')[['State', 'Bootstrap_Mean_Width', 'Parametric_Mean_Width', 'Width_Difference']]\ndisplay(extreme_diff.round(4))\n\nprint(f\"\\nAverage width ratio (Bootstrap/Parametric): {comparison_df['Width_Ratio'].mean():.3f}\")\nprint(f\"Bootstrap wider in {sum(comparison_df['Width_Ratio'] > 1)} of {len(comparison_df)} states\")\n\nprint(\"\\n\" + \"=\"*80 + \"\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "uqgijtdyc5",
   "source": "# Visualize confidence intervals\nfig, axes = plt.subplots(2, 2, figsize=(18, 12))\n\n# 1. Top 15 states - Bootstrap confidence intervals\nax1 = axes[0, 0]\ntop15 = confidence_df.head(15)\ny_pos = np.arange(len(top15))\n\nax1.errorbar(top15['Mean'], y_pos, \n            xerr=top15['Bootstrap_Mean_Width']/2,\n            fmt='o', capsize=3, capthick=1, markersize=4)\nax1.set_yticks(y_pos)\nax1.set_yticklabels(top15['State'])\nax1.set_xlabel('Mean Income (MW)')\nax1.set_title('Top 15 States: Mean Income with 95% CI (Bootstrap)')\nax1.grid(True, alpha=0.3)\nax1.invert_yaxis()\n\n# 2. Bottom 15 states - Bootstrap confidence intervals\nax2 = axes[0, 1]\nbottom15 = confidence_df.tail(15)\ny_pos_b = np.arange(len(bottom15))\n\nax2.errorbar(bottom15['Mean'], y_pos_b, \n            xerr=bottom15['Bootstrap_Mean_Width']/2,\n            fmt='o', capsize=3, capthick=1, markersize=4, color='red')\nax2.set_yticks(y_pos_b)\nax2.set_yticklabels(bottom15['State'])\nax2.set_xlabel('Mean Income (MW)')\nax2.set_title('Bottom 15 States: Mean Income with 95% CI (Bootstrap)')\nax2.grid(True, alpha=0.3)\nax2.invert_yaxis()\n\n# 3. Confidence interval width vs sample size\nax3 = axes[1, 0]\nax3.scatter(confidence_df['Sample_Size'], confidence_df['Bootstrap_Mean_Width'], \n           alpha=0.6, s=30)\nax3.set_xlabel('Sample Size')\nax3.set_ylabel('CI Width (MW)')\nax3.set_title('Confidence Interval Width vs Sample Size')\nax3.set_xscale('log')\nax3.grid(True, alpha=0.3)\n\n# Add trend line\nz = np.polyfit(np.log(confidence_df['Sample_Size']), confidence_df['Bootstrap_Mean_Width'], 1)\np = np.poly1d(z)\nax3.plot(confidence_df['Sample_Size'], p(np.log(confidence_df['Sample_Size'])), \"r--\", alpha=0.8)\n\n# 4. Bootstrap vs Parametric CI comparison\nax4 = axes[1, 1]\nax4.scatter(confidence_df['Parametric_Mean_Width'], confidence_df['Bootstrap_Mean_Width'], \n           alpha=0.6, s=30)\nax4.plot([0, confidence_df['Parametric_Mean_Width'].max()], \n         [0, confidence_df['Parametric_Mean_Width'].max()], 'r--', alpha=0.8)\nax4.set_xlabel('Parametric CI Width (MW)')\nax4.set_ylabel('Bootstrap CI Width (MW)')\nax4.set_title('Bootstrap vs Parametric CI Width Comparison')\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Statistical significance testing between states\nprint(\"4. STATISTICAL SIGNIFICANCE TESTING BETWEEN STATES\")\nprint(\"-\" * 80)\n\n# Test if top 3 states are significantly different from bottom 3 states\ntop3_states = confidence_df.head(3)['State'].values\nbottom3_states = confidence_df.tail(3)['State'].values\n\nprint(\"Comparing TOP 3 vs BOTTOM 3 states:\")\nprint(f\"Top 3: {', '.join(top3_states)}\")\nprint(f\"Bottom 3: {', '.join(bottom3_states)}\")\n\n# Perform Mann-Whitney U test\ntop3_income = df_income[df_income['UF_label'].isin(top3_states)][mw_col]\nbottom3_income = df_income[df_income['UF_label'].isin(bottom3_states)][mw_col]\n\nmw_stat, mw_p = mannwhitneyu(top3_income, bottom3_income, alternative='two-sided')\n\nprint(f\"\\nMann-Whitney U Test:\")\nprint(f\"  Statistic: {mw_stat}\")\nprint(f\"  P-value: {mw_p:.2e}\")\nprint(f\"  Result: {'Significantly different' if mw_p < 0.05 else 'Not significantly different'}\")\n\n# Effect size (Cohen's d)\ntop3_mean = top3_income.mean()\nbottom3_mean = bottom3_income.mean()\npooled_std = np.sqrt(((len(top3_income) - 1) * top3_income.var() + \n                     (len(bottom3_income) - 1) * bottom3_income.var()) / \n                    (len(top3_income) + len(bottom3_income) - 2))\ncohens_d = (top3_mean - bottom3_mean) / pooled_std\n\nprint(f\"\\nEffect Size (Cohen's d): {cohens_d:.3f}\")\nmagnitude = 'small' if abs(cohens_d) < 0.5 else 'medium' if abs(cohens_d) < 0.8 else 'large'\nprint(f\"Magnitude: {magnitude}\")\n\nprint(f\"\\nDescriptive Statistics:\")\nprint(f\"  Top 3 states mean: {top3_mean:.3f} MW (n={len(top3_income)})\")\nprint(f\"  Bottom 3 states mean: {bottom3_mean:.3f} MW (n={len(bottom3_income)})\")\nprint(f\"  Difference: {top3_mean - bottom3_mean:.3f} MW ({((top3_mean/bottom3_mean - 1)*100):+.1f}%)\")\n\nprint(\"\\n\" + \"=\"*80 + \"\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "y40mps0goq",
   "source": "## 5. Multivariate Analysis for Income Determinants\n\nAn\u00e1lise multivariada para identificar os principais determinantes da renda, incluindo regress\u00e3o m\u00faltipla, PCA e an\u00e1lise de clusters.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "v8amnsn4pq",
   "source": "import statsmodels.api as sm\n# Prepare comprehensive dataset for multivariate analysis\nprint(\"=== MULTIVARIATE ANALYSIS FOR INCOME DETERMINANTS ===\\n\")\n\n# Select relevant variables for analysis\nmultivar_columns = [\n    # Demographics\n    'V2007__sexo',  # Gender\n    'V2009__idade_na_data_de_referncia',  # Age\n    'V2010__cor_ou_raa',  # Race\n    \n    # Education\n    'V3001__sabe_ler_e_escrever',  # Literacy\n    'V3009A__curso_mais_elevado_que_frequentou',  # Education level\n    \n    # Geography\n    'UF__unidade_da_federao',  # State\n    'Capital__municpio_da_capital',  # Capital city\n    \n    # Household\n    'VD2003__nmero_de_componentes_do_domic',  # Household size\n    'V2005__condio_no_domiclio',  # Household position\n    \n    # Work-related (if available)\n    'VD4001__condicao_de_forca_de_trabalho',  # Labor force status\n    'VD4002__condicao_de_ocupacao',  # Employment status\n]\n\n# Keep only columns that exist in the dataset\n# Filter to only existing columns\navailable_columns = [col for col in multivar_columns if col in df.columns]\nanalysis_df = df[available_columns + [mw_col]].copy()\n\n# Clean data - remove rows with missing income\nanalysis_df = analysis_df[analysis_df[mw_col].notna() & (analysis_df[mw_col] > 0)]\n\nprint(f\"Dataset for multivariate analysis:\")\nprint(f\"  Total observations: {len(analysis_df):,}\")\nprint(f\"  Available variables: {len(available_columns)}\")\nprint(f\"  Variables: {available_columns}\")\n\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\n# 1. MULTIPLE LINEAR REGRESSION ANALYSIS\nprint(\"1. MULTIPLE LINEAR REGRESSION ANALYSIS\")\nprint(\"-\" * 50)\n\n# Prepare features for regression\nreg_df = analysis_df.copy()\n\n# Create dummy variables for categorical features\ncategorical_features = []\nnumerical_features = ['V2009__idade_na_data_de_referncia', 'VD2003__nmero_de_componentes_do_domic']\n\nfor col in available_columns:\n    if col not in numerical_features:\n        # Create dummy variables\n        dummies = pd.get_dummies(reg_df[col], prefix=col[:10], dummy_na=False)\n        categorical_features.extend(dummies.columns.tolist())\n        reg_df = pd.concat([reg_df, dummies], axis=1)\n\n# Select features for regression (limit to avoid overfitting)\nall_features = numerical_features + categorical_features\nX_reg = reg_df[all_features].fillna(0)  # Fill any remaining NaN with 0\ny_reg = reg_df[mw_col]\n\n# Add constant for regression\nX_reg_const = sm.add_constant(X_reg)\n\n# Fit multiple regression model\ntry:\n    reg_model = sm.OLS(y_reg, X_reg_const).fit()\n    \n    print(f\"Regression Results:\")\n    print(f\"  R-squared: {reg_model.rsquared:.4f}\")\n    print(f\"  Adjusted R-squared: {reg_model.rsquared_adj:.4f}\")\n    print(f\"  F-statistic: {reg_model.fvalue:.2f}\")\n    print(f\"  F-statistic p-value: {reg_model.f_pvalue:.2e}\")\n    \n    # Get top 10 most significant coefficients\n    coeffs = pd.DataFrame({\n        'Variable': reg_model.params.index,\n        'Coefficient': reg_model.params.values,\n        'P-value': reg_model.pvalues.values,\n        'Significant': reg_model.pvalues.values < 0.05\n    })\n    coeffs = coeffs[coeffs['Variable'] != 'const']  # Remove intercept\n    coeffs['Abs_Coefficient'] = abs(coeffs['Coefficient'])\n    \n    # Top 10 by absolute coefficient value\n    print(f\"\\nTop 10 Variables by Coefficient Magnitude:\")\n    top_coeffs = coeffs.nlargest(10, 'Abs_Coefficient')[['Variable', 'Coefficient', 'P-value', 'Significant']]\n    for _, row in top_coeffs.iterrows():\n        sig_marker = \"***\" if row['P-value'] < 0.001 else \"**\" if row['P-value'] < 0.01 else \"*\" if row['P-value'] < 0.05 else \"\"\n        print(f\"  {row['Variable'][:30]:30} | Coeff: {row['Coefficient']:+7.4f} | p: {row['P-value']:.3e} {sig_marker}\")\n    \n    # Model diagnostics\n    print(f\"\\nModel Diagnostics:\")\n    print(f\"  Number of observations: {int(reg_model.nobs)}\")\n    print(f\"  Number of parameters: {len(reg_model.params)}\")\n    print(f\"  Log-likelihood: {reg_model.llf:.2f}\")\n    print(f\"  AIC: {reg_model.aic:.2f}\")\n    print(f\"  BIC: {reg_model.bic:.2f}\")\n    \nexcept Exception as e:\n    print(f\"Regression analysis failed: {e}\")\n    print(\"This may be due to multicollinearity or insufficient data.\")\n\nprint(\"\\n\" + \"=\"*80 + \"\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "gvd8x104iwd",
   "source": "# 2. PRINCIPAL COMPONENT ANALYSIS (PCA)\nprint(\"2. PRINCIPAL COMPONENT ANALYSIS (PCA)\")\nprint(\"-\" * 50)\n\n# Prepare data for PCA - use only numerical features and encoded categoricals\npca_df = analysis_df.copy()\n\n# Encode categorical variables numerically\nlabel_encoders_pca = {}\nfor col in available_columns:\n    if col not in numerical_features:\n        le = LabelEncoder()\n        pca_df[col] = le.fit_transform(pca_df[col].astype(str))\n        label_encoders_pca[col] = le\n\n# Select features for PCA\npca_features = available_columns\nX_pca = pca_df[pca_features].fillna(pca_df[pca_features].median())\n\n# Standardize features\nscaler_pca = StandardScaler()\nX_pca_scaled = scaler_pca.fit_transform(X_pca)\n\n# Perform PCA\npca = PCA()\nX_pca_transformed = pca.fit_transform(X_pca_scaled)\n\n# Calculate variance explained\nvariance_explained = pca.explained_variance_ratio_\ncumulative_variance = np.cumsum(variance_explained)\n\nprint(f\"PCA Results:\")\nprint(f\"  Number of components: {len(variance_explained)}\")\nprint(f\"  Variance explained by first 5 components:\")\nfor i in range(min(5, len(variance_explained))):\n    print(f\"    PC{i+1}: {variance_explained[i]:.4f} ({cumulative_variance[i]:.4f} cumulative)\")\n\n# Number of components for 80% and 90% variance\npc80 = np.argmax(cumulative_variance >= 0.80) + 1\npc90 = np.argmax(cumulative_variance >= 0.90) + 1\nprint(f\"  Components for 80% variance: {pc80}\")\nprint(f\"  Components for 90% variance: {pc90}\")\n\n# Component loadings for first 3 PCs\nprint(f\"\\nComponent Loadings (First 3 PCs):\")\nloadings_df = pd.DataFrame(\n    pca.components_[:3].T,\n    columns=['PC1', 'PC2', 'PC3'],\n    index=pca_features\n)\n\n# Show top loadings for each component\nfor pc in ['PC1', 'PC2', 'PC3']:\n    print(f\"\\n{pc} - Top 5 positive and negative loadings:\")\n    sorted_loadings = loadings_df[pc].sort_values(key=abs, ascending=False)\n    for var, loading in sorted_loadings.head(5).items():\n        print(f\"  {var[:25]:25} | {loading:+.4f}\")\n\n# Correlation between PCs and income\npc_income_corr = []\nfor i in range(min(10, len(variance_explained))):\n    corr = np.corrcoef(X_pca_transformed[:, i], pca_df[mw_col])[0, 1]\n    pc_income_corr.append(corr)\n\nprint(f\"\\nCorrelation between Principal Components and Income:\")\nfor i, corr in enumerate(pc_income_corr):\n    print(f\"  PC{i+1}: {corr:+.4f}\")\n\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\n# 3. CLUSTER ANALYSIS\nprint(\"3. CLUSTER ANALYSIS\")\nprint(\"-\" * 50)\n\n# Use first few principal components for clustering\nn_components_cluster = min(pc80, 10)  # Use components that explain 80% variance, max 10\nX_cluster = X_pca_transformed[:, :n_components_cluster]\n\nprint(f\"Using {n_components_cluster} principal components for clustering\")\n\n# Determine optimal number of clusters using elbow method\nmax_clusters = 10\ninertias = []\nsilhouette_scores = []\n\nfor k in range(2, max_clusters + 1):\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    cluster_labels = kmeans.fit_predict(X_cluster)\n    inertias.append(kmeans.inertia_)\n    \n    # Calculate silhouette score\n    from sklearn.metrics import silhouette_score\n    silhouette = silhouette_score(X_cluster, cluster_labels)\n    silhouette_scores.append(silhouette)\n\n# Find optimal number of clusters\noptimal_k = np.argmax(silhouette_scores) + 2  # +2 because range starts at 2\n\nprint(f\"Optimal number of clusters (silhouette score): {optimal_k}\")\nprint(f\"Silhouette scores:\")\nfor i, score in enumerate(silhouette_scores, 2):\n    print(f\"  k={i}: {score:.4f}\")\n\n# Perform final clustering\nkmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\ncluster_labels = kmeans_final.fit_predict(X_cluster)\npca_df['cluster'] = cluster_labels\n\n# Analyze clusters\nprint(f\"\\nCluster Analysis Results:\")\ncluster_analysis = pca_df.groupby('cluster').agg({\n    mw_col: ['count', 'mean', 'median', 'std'],\n    'V2009__idade_na_data_de_referncia': 'mean',\n    'VD2003__nmero_de_componentes_do_domic': 'mean'\n}).round(3)\n\ncluster_analysis.columns = ['Count', 'Income_Mean', 'Income_Median', 'Income_Std', 'Age_Mean', 'HH_Size_Mean']\ndisplay(cluster_analysis)\n\n# Cluster characteristics\nprint(f\"\\nCluster Characteristics:\")\nfor cluster_id in range(optimal_k):\n    cluster_data = pca_df[pca_df['cluster'] == cluster_id]\n    print(f\"\\nCluster {cluster_id} (n={len(cluster_data)}):\")\n    print(f\"  Mean income: {cluster_data[mw_col].mean():.3f} MW\")\n    print(f\"  Mean age: {cluster_data['V2009__idade_na_data_de_referncia'].mean():.1f} years\")\n    print(f\"  Mean household size: {cluster_data['VD2003__nmero_de_componentes_do_domic'].mean():.1f}\")\n    \n    # Most common categorical values in cluster\n    for col in ['V2007__sexo', 'V2010__cor_ou_raa']:\n        if col in available_columns:\n            most_common = cluster_data[col].mode().iloc[0] if not cluster_data[col].empty else 'N/A'\n            print(f\"  Most common {col.split('__')[1]}: {most_common}\")\n\nprint(\"\\n\" + \"=\"*80 + \"\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qg2vk0g9r7",
   "source": "# 4. VISUALIZATION OF MULTIVARIATE RESULTS\nprint(\"4. VISUALIZATION OF MULTIVARIATE RESULTS\")\nprint(\"-\" * 50)\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\n\n# 1. PCA Scree Plot\nax1 = axes[0, 0]\nax1.plot(range(1, min(11, len(variance_explained) + 1)), variance_explained[:10], 'bo-', linewidth=2, markersize=6)\nax1.plot(range(1, min(11, len(variance_explained) + 1)), cumulative_variance[:10], 'ro-', linewidth=2, markersize=6)\nax1.set_xlabel('Principal Component')\nax1.set_ylabel('Proportion of Variance Explained')\nax1.set_title('PCA Scree Plot')\nax1.legend(['Individual', 'Cumulative'])\nax1.grid(True, alpha=0.3)\nax1.axhline(y=0.8, color='g', linestyle='--', alpha=0.7, label='80% line')\n\n# 2. PC1 vs PC2 Scatter Plot colored by income\nax2 = axes[0, 1]\nscatter = ax2.scatter(X_pca_transformed[:, 0], X_pca_transformed[:, 1], \n                     c=pca_df[mw_col], cmap='viridis', alpha=0.6, s=10)\nax2.set_xlabel(f'PC1 ({variance_explained[0]:.3f} variance)')\nax2.set_ylabel(f'PC2 ({variance_explained[1]:.3f} variance)')\nax2.set_title('PCA: PC1 vs PC2 (colored by income)')\nplt.colorbar(scatter, ax=ax2, label='Income (MW)')\n\n# 3. Cluster Analysis - PC1 vs PC2 colored by cluster\nax3 = axes[0, 2]\ncolors = plt.cm.Set3(np.linspace(0, 1, optimal_k))\nfor i in range(optimal_k):\n    cluster_mask = pca_df['cluster'] == i\n    ax3.scatter(X_pca_transformed[cluster_mask, 0], X_pca_transformed[cluster_mask, 1],\n               c=[colors[i]], label=f'Cluster {i}', alpha=0.6, s=10)\nax3.set_xlabel(f'PC1 ({variance_explained[0]:.3f} variance)')\nax3.set_ylabel(f'PC2 ({variance_explained[1]:.3f} variance)')\nax3.set_title('Cluster Analysis: PC1 vs PC2')\nax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# 4. Income distribution by cluster\nax4 = axes[1, 0]\ncluster_income_data = [pca_df[pca_df['cluster'] == i][mw_col] for i in range(optimal_k)]\nax4.boxplot(cluster_income_data, labels=[f'C{i}' for i in range(optimal_k)])\nax4.set_xlabel('Cluster')\nax4.set_ylabel('Income (MW)')\nax4.set_title('Income Distribution by Cluster')\nax4.grid(True, alpha=0.3)\n\n# 5. Elbow plot for clustering\nax5 = axes[1, 1]\nk_range = range(2, max_clusters + 1)\nax5.plot(k_range, inertias, 'bo-', linewidth=2, markersize=6)\nax5.set_xlabel('Number of Clusters (k)')\nax5.set_ylabel('Within-cluster Sum of Squares (WCSS)')\nax5.set_title('Elbow Method for Optimal k')\nax5.grid(True, alpha=0.3)\nax5.axvline(x=optimal_k, color='r', linestyle='--', alpha=0.7, label=f'Optimal k={optimal_k}')\nax5.legend()\n\n# 6. Silhouette scores\nax6 = axes[1, 2]\nax6.plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=6)\nax6.set_xlabel('Number of Clusters (k)')\nax6.set_ylabel('Silhouette Score')\nax6.set_title('Silhouette Analysis')\nax6.grid(True, alpha=0.3)\nax6.axvline(x=optimal_k, color='r', linestyle='--', alpha=0.7, label=f'Optimal k={optimal_k}')\nax6.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\n# 5. SUMMARY OF KEY FINDINGS\nprint(\"5. SUMMARY OF KEY INCOME DETERMINANTS\")\nprint(\"-\" * 50)\n\n# Compile key findings from all analyses\nkey_findings = {\n    'Inequality Metrics': {\n        'Gini Coefficient (Individual)': f\"{gini_individual_mw:.4f}\",\n        'Gini Coefficient (Household)': f\"{gini_household_mw:.4f}\",\n        'Top 10% Income Share': f\"{decile_shares[-1]:.1f}%\",\n        'Bottom 50% Income Share': f\"{decile_shares[:5].sum():.1f}%\"\n    }\n}\n\nif 'reg_model' in locals():\n    # Top significant predictors from regression\n    significant_vars = coeffs[coeffs['Significant'] == True].nlargest(5, 'Abs_Coefficient')\n    key_findings['Top Regression Predictors'] = {\n        row['Variable'][:30]: f\"{row['Coefficient']:+.4f}\" \n        for _, row in significant_vars.iterrows()\n    }\n    key_findings['Model Performance'] = {\n        'R-squared': f\"{reg_model.rsquared:.4f}\",\n        'Adjusted R-squared': f\"{reg_model.rsquared_adj:.4f}\"\n    }\n\n# PCA findings\nkey_findings['Dimensionality Reduction'] = {\n    'Components for 80% variance': f\"{pc80}\",\n    'First PC variance explained': f\"{variance_explained[0]:.4f}\",\n    'PC1-Income correlation': f\"{pc_income_corr[0]:+.4f}\"\n}\n\n# Cluster findings\nkey_findings['Population Segments'] = {\n    'Optimal number of clusters': f\"{optimal_k}\",\n    'Highest income cluster mean': f\"{cluster_analysis['Income_Mean'].max():.3f} MW\",\n    'Lowest income cluster mean': f\"{cluster_analysis['Income_Mean'].min():.3f} MW\",\n    'Income ratio (highest/lowest)': f\"{cluster_analysis['Income_Mean'].max() / cluster_analysis['Income_Mean'].min():.2f}\"\n}\n\n# Statistical test findings\nkey_findings['Statistical Significance'] = {\n    'Gender income difference': f\"{'Significant' if gender_tests['kruskal_wallis']['significant'] else 'Not significant'}\",\n    'Race income difference': f\"{'Significant' if race_tests['kruskal_wallis']['significant'] else 'Not significant'}\",\n    'Age group income difference': f\"{'Significant' if age_tests['kruskal_wallis']['significant'] else 'Not significant'}\",\n    'State income difference': f\"{'Significant' if state_tests['kruskal_wallis']['significant'] else 'Not significant'}\"\n}\n\nprint(\"KEY FINDINGS SUMMARY:\")\nfor category, findings in key_findings.items():\n    print(f\"\\n{category}:\")\n    for key, value in findings.items():\n        print(f\"  \u2022 {key}: {value}\")\n\nprint(f\"\\nDATA COVERAGE:\")\nprint(f\"  \u2022 Total individuals analyzed: {len(df_income):,}\")\nprint(f\"  \u2022 Total households analyzed: {len(household_income):,}\")\nprint(f\"  \u2022 States with sufficient data: {len(states_with_sufficient_data)}\")\nprint(f\"  \u2022 Analysis period: {df['ym'].min()} to {df['ym'].max()}\")\n\nprint(f\"\\nRECOMMENDations FOR POLICY:\")\nprint(f\"  \u2022 High inequality (Gini > 0.5) suggests need for redistribution policies\")\nprint(f\"  \u2022 Significant demographic differences indicate targeted interventions needed\")\nprint(f\"  \u2022 Geographic disparities require regional development focus\")\nprint(f\"  \u2022 Multivariate analysis reveals key leverage points for income improvement\")\n\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\nprint(\"ADVANCED STATISTICAL ANALYSIS COMPLETED SUCCESSFULLY!\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v2iio8lw3b",
   "source": "## \ud83d\udcca Visualiza\u00e7\u00f5es Avan\u00e7adas - PNADC\n\nEste conjunto de visualiza\u00e7\u00f5es oferece uma an\u00e1lise completa dos dados de renda brasileiros da PNADC:\n\n### \ud83d\uddfa\ufe0f **1. Mapa Coropl\u00e9tico Interativo**\n- Visualiza\u00e7\u00e3o geogr\u00e1fica da renda mediana por estado\n- Cores representam diferentes faixas de renda\n- Hover interativo com detalhes por estado\n- Compat\u00edvel com dados geogr\u00e1ficos do IBGE\n\n### \ud83d\udc65 **2. Pir\u00e2mides Demogr\u00e1ficas com Sobreposi\u00e7\u00e3o de Renda**\n- Distribui\u00e7\u00e3o populacional por idade e sexo\n- Camadas de cor representando faixas de renda\n- Identifica\u00e7\u00e3o de padr\u00f5es demogr\u00e1ficos e econ\u00f4micos\n- Compara\u00e7\u00e3o entre homens e mulheres\n\n### \ud83d\udcc8 **3. S\u00e9ries Temporais de Renda**\n- Evolu\u00e7\u00e3o da renda ao longo dos trimestres\n- Linhas de mediana e m\u00e9dia com bandas de confian\u00e7a\n- Identifica\u00e7\u00e3o de tend\u00eancias e sazonalidades\n- Anota\u00e7\u00f5es autom\u00e1ticas para insights chave\n\n### \ud83c\udf0a **4. Diagrama Sankey de Fluxo de Renda**\n- Fluxo entre educa\u00e7\u00e3o \u2192 idade \u2192 faixa de renda\n- Identifica\u00e7\u00e3o de caminhos de mobilidade social\n- Cores categ\u00f3ricas para diferentes grupos\n- Espessura dos fluxos representa volume populacional\n\n### \ud83d\udd25 **5. Heatmaps de Correla\u00e7\u00e3o Avan\u00e7ados**\n- Matriz de correla\u00e7\u00e3o entre vari\u00e1veis principais\n- Ranking de correla\u00e7\u00f5es com renda\n- Perfil demogr\u00e1fico por estado\n- Vers\u00f5es est\u00e1ticas (matplotlib/seaborn) e interativas (plotly)\n\n### \ud83c\udf9b\ufe0f **6. Dashboard Interativo Resumo**\n- Combina\u00e7\u00e3o de m\u00faltiplas visualiza\u00e7\u00f5es em uma tela\n- Compara\u00e7\u00f5es r\u00e1pidas entre estados\n- Distribui\u00e7\u00f5es e correla\u00e7\u00f5es lado a lado\n- Layout responsivo e profissional\n\n---\n\n### \ud83c\udfa8 **Caracter\u00edsticas T\u00e9cnicas:**\n- **Paletas de cores**: Adequadas para dados de renda (RdYlGn, viridis, RdYlBu)\n- **Interatividade**: Hover, zoom, sele\u00e7\u00e3o em gr\u00e1ficos Plotly\n- **Responsividade**: Layouts que se adaptam a diferentes tamanhos\n- **Acessibilidade**: Contrastes adequados e labels em portugu\u00eas\n- **Performance**: Otimizadas para datasets grandes\n\n### \ud83d\ude80 **Como usar:**\n1. Execute as c\u00e9lulas sequencialmente\n2. As visualiza\u00e7\u00f5es aparecer\u00e3o automaticamente\n3. Gr\u00e1ficos interativos permitem explora\u00e7\u00e3o detalhada\n4. Dados processados ficam dispon\u00edveis para an\u00e1lises adicionais\n\n---\n*\ud83d\udca1 **Dica**: Para melhor experi\u00eancia, execute em um ambiente com boa conex\u00e3o para carregar dados geogr\u00e1ficos e bibliotecas de visualiza\u00e7\u00e3o.*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "46ieejb461p",
   "source": "# 6. Bonus: Advanced Interactive Dashboard Summary\ndef create_dashboard_summary(df, state_income_summary, corr_matrix):\n    \"\"\"\n    Create a comprehensive dashboard summary combining all visualizations.\n    \"\"\"\n    # Create subplots for dashboard\n    fig = make_subplots(\n        rows=2, cols=3,\n        subplot_titles=(\n            'Distribui\u00e7\u00e3o de Renda por Estado',\n            'Renda por Faixa Et\u00e1ria e Sexo', \n            'Correla\u00e7\u00f5es com Renda',\n            'Renda Mediana vs M\u00e9dia',\n            'Distribui\u00e7\u00e3o de Renda',\n            'Top Estados por Renda'\n        ),\n        specs=[[{\"type\": \"bar\"}, {\"type\": \"box\"}, {\"type\": \"scatter\"}],\n               [{\"type\": \"scatter\"}, {\"type\": \"histogram\"}, {\"type\": \"bar\"}]],\n        vertical_spacing=0.15,\n        horizontal_spacing=0.1\n    )\n    \n    # 1. Top states by income\n    top_states = state_income_summary.nlargest(10, 'Renda_Mediana_SM')\n    fig.add_trace(\n        go.Bar(x=top_states['Renda_Mediana_SM'], \n               y=top_states['Estado'], \n               orientation='h',\n               marker_color='lightblue',\n               name='Renda Mediana'),\n        row=1, col=1\n    )\n    \n    # 2. Income by age and sex\n    if 'V2007' in df.columns:\n        age_groups = pd.cut(df['V2009'], bins=[0, 25, 35, 45, 55, 100], \n                           labels=['18-24', '25-34', '35-44', '45-54', '55+'])\n        for sex in df['V2007'].unique()[:2]:  # Limit to 2 categories\n            sex_data = df[df['V2007'] == sex]\n            fig.add_trace(\n                go.Box(y=sex_data[INCOME_COL], \n                       name=f'{sex}',\n                       boxpoints='outliers'),\n                row=1, col=2\n            )\n    \n    # 3. Correlation with income\n    if corr_matrix is not None:\n        income_correlations = corr_matrix[INCOME_COL].abs().sort_values(ascending=False)[1:6]\n        fig.add_trace(\n            go.Scatter(x=list(range(len(income_correlations))), \n                      y=income_correlations.values,\n                      mode='markers+lines',\n                      marker=dict(size=10, color='red'),\n                      name='Correla\u00e7\u00e3o'),\n            row=1, col=3\n        )\n    \n    # 4. Median vs Mean income scatter\n    fig.add_trace(\n        go.Scatter(x=state_income_summary['Renda_Mediana_SM'], \n                  y=state_income_summary['Renda_Media_SM'],\n                  mode='markers+text',\n                  text=state_income_summary['Estado'],\n                  textposition='top center',\n                  marker=dict(size=8, color='green'),\n                  name='Estados'),\n        row=2, col=1\n    )\n    \n    # 5. Income distribution\n    fig.add_trace(\n        go.Histogram(x=df[INCOME_COL], \n                    nbinsx=30,\n                    marker_color='orange',\n                    name='Distribui\u00e7\u00e3o'),\n        row=2, col=2\n    )\n    \n    # 6. Bottom states for comparison\n    bottom_states = state_income_summary.nsmallest(5, 'Renda_Mediana_SM')\n    fig.add_trace(\n        go.Bar(x=bottom_states['Renda_Mediana_SM'], \n               y=bottom_states['Estado'], \n               orientation='h',\n               marker_color='lightcoral',\n               name='Menores Rendas'),\n        row=2, col=3\n    )\n    \n    # Update layout\n    fig.update_layout(\n        height=800,\n        showlegend=False,\n        title_text=\"Dashboard PNADC - An\u00e1lise de Renda Brasileira\",\n        title_x=0.5,\n        title_font_size=18,\n        font_family=\"Arial\"\n    )\n    \n    # Update axes labels\n    fig.update_xaxes(title_text=\"Renda (SM)\", row=1, col=1)\n    fig.update_xaxes(title_text=\"Renda (SM)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Renda (SM)\", row=2, col=1)\n    fig.update_xaxes(title_text=\"Renda (SM)\", row=2, col=2)\n    \n    return fig\n\n# Create dashboard summary\ntry:\n    fig_dashboard = create_dashboard_summary(df, state_income_summary, corr_matrix)\n    fig_dashboard.show()\n    print(\"\u2705 Dashboard interativo criado com sucesso!\")\nexcept Exception as e:\n    print(f\"\u26a0\ufe0f  Erro ao criar dashboard: {e}\")\n    print(\"Isso pode acontecer se algumas vari\u00e1veis n\u00e3o estiverem dispon\u00edveis no dataset.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "amcbz3altpb",
   "source": "# 5. Advanced Heatmaps for Correlation Analysis\ndef create_income_correlation_heatmaps(df):\n    \"\"\"\n    Create advanced heatmap visualizations for income correlation analysis.\n    \"\"\"\n    # Prepare numeric variables for correlation analysis\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    \n    # Remove ID columns and irrelevant ones\n    exclude_patterns = ['id', 'ID', 'codigo', 'sequencial', 'dom_id']\n    numeric_cols = [col for col in numeric_cols if not any(pattern in col for pattern in exclude_patterns)]\n    \n    # Ensure we have the income column\n    if INCOME_COL not in numeric_cols:\n        print(f\"Warning: {INCOME_COL} not found in numeric columns\")\n        return None, None\n    \n    # Limit to top correlated variables with income for readability\n    correlations_with_income = df[numeric_cols].corrwith(df[INCOME_COL]).abs().sort_values(ascending=False)\n    top_corr_vars = correlations_with_income.head(15).index.tolist()\n    \n    print(\"\ud83d\udd0d Vari\u00e1veis mais correlacionadas com renda:\")\n    for var in top_corr_vars[:10]:\n        corr_value = correlations_with_income[var]\n        print(f\"  {var}: {corr_value:.3f}\")\n    \n    # Create correlation matrix\n    correlation_matrix = df[top_corr_vars].corr()\n    \n    # 1. Basic Correlation Heatmap\n    fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n    \n    # Correlation heatmap\n    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n    sns.heatmap(correlation_matrix, \n                mask=mask,\n                annot=True, \n                cmap='RdYlBu_r', \n                center=0,\n                square=True,\n                fmt='.2f',\n                cbar_kws={\"shrink\": .8},\n                ax=ax1)\n    ax1.set_title('Matriz de Correla\u00e7\u00e3o - Vari\u00e1veis vs Renda', fontsize=14, pad=20)\n    ax1.tick_params(axis='x', rotation=45)\n    ax1.tick_params(axis='y', rotation=0)\n    \n    # Income correlation bar plot\n    income_correlations = correlations_with_income.head(10)\n    colors = plt.cm.RdYlBu_r(np.linspace(0, 1, len(income_correlations)))\n    bars = ax2.barh(range(len(income_correlations)), income_correlations.values, color=colors)\n    ax2.set_yticks(range(len(income_correlations)))\n    ax2.set_yticklabels([col.replace('__', '\\n').replace('_', ' ') for col in income_correlations.index])\n    ax2.set_xlabel('Correla\u00e7\u00e3o Absoluta com Renda')\n    ax2.set_title('Correla\u00e7\u00f5es Mais Fortes com Renda', fontsize=14, pad=20)\n    ax2.grid(axis='x', alpha=0.3)\n    \n    # Add values on bars\n    for i, bar in enumerate(bars):\n        width = bar.get_width()\n        ax2.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n                f'{width:.3f}', ha='left', va='center')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 2. Interactive Plotly Heatmap\n    fig2 = go.Figure(data=go.Heatmap(\n        z=correlation_matrix.values,\n        x=[col.replace('__', '<br>').replace('_', ' ') for col in correlation_matrix.columns],\n        y=[col.replace('__', '<br>').replace('_', ' ') for col in correlation_matrix.index],\n        colorscale='RdYlBu',\n        zmid=0,\n        text=correlation_matrix.values,\n        texttemplate=\"%{text:.2f}\",\n        textfont={\"size\": 10},\n        hoverongaps=False,\n        hovertemplate='<b>%{y}</b><br><b>%{x}</b><br>Correla\u00e7\u00e3o: %{z:.3f}<extra></extra>'\n    ))\n    \n    fig2.update_layout(\n        title='Matriz de Correla\u00e7\u00e3o Interativa - Vari\u00e1veis PNADC',\n        title_x=0.5,\n        font_family=\"Arial\",\n        height=700,\n        width=800\n    )\n    \n    fig2.show()\n    \n    # 3. State-Income Heatmap\n    state_demographics = df.groupby('UF_label').agg({\n        INCOME_COL: ['mean', 'median'],\n        'V2009': 'mean',  # Average age\n        'V2007': lambda x: (x == 'Mulher').mean() if 'V2007' in df.columns else 0.5,  # % women\n    }).round(3)\n    \n    state_demographics.columns = ['Renda_Media', 'Renda_Mediana', 'Idade_Media', 'Perc_Mulheres']\n    state_demographics = state_demographics.reset_index()\n    \n    # Pivot for heatmap\n    state_metrics = state_demographics.set_index('UF_label')\n    \n    # Normalize for better visualization\n    state_metrics_norm = (state_metrics - state_metrics.min()) / (state_metrics.max() - state_metrics.min())\n    \n    fig3 = plt.figure(figsize=(12, 10))\n    sns.heatmap(state_metrics_norm.T, \n                annot=state_metrics.T, \n                fmt='.2f',\n                cmap='viridis',\n                cbar_kws={'label': 'Valores Normalizados'},\n                square=False)\n    plt.title('Perfil Demogr\u00e1fico e de Renda por Estado (PNADC)', fontsize=14, pad=20)\n    plt.xlabel('Estados')\n    plt.ylabel('Indicadores')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.show()\n    \n    return correlation_matrix, state_demographics\n\n# Create correlation heatmaps\ncorr_matrix, state_demo = create_income_correlation_heatmaps(df)\nprint(\"\\n\u2705 Heatmaps de correla\u00e7\u00e3o criados com sucesso!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v3oi78lu88",
   "source": "# 4. Sankey Diagram: Income Flow Between Brackets\ndef create_income_sankey_diagram(df):\n    \"\"\"\n    Create a Sankey diagram showing income flow between demographic groups and income brackets.\n    \"\"\"\n    # Create age groups and income brackets\n    df_viz = df.copy()\n    df_viz['Idade_Grupo'] = pd.cut(df_viz['V2009'], \n                                   bins=[0, 25, 35, 45, 55, 100], \n                                   labels=['Jovens (0-24)', 'Adultos Jovens (25-34)', \n                                          'Adultos (35-44)', 'Maduros (45-54)', 'Seniores (55+)'])\n    \n    # Create income brackets based on quantiles\n    income_quantiles = df_viz[INCOME_COL].quantile([0.2, 0.4, 0.6, 0.8]).values\n    df_viz['Faixa_Renda'] = pd.cut(df_viz[INCOME_COL], \n                                   bins=[0] + list(income_quantiles) + [df_viz[INCOME_COL].max()],\n                                   labels=['Muito Baixa', 'Baixa', 'M\u00e9dia', 'Alta', 'Muito Alta'])\n    \n    # Prepare education levels (assuming VD3006 is education)\n    education_cols = [col for col in df.columns if 'educ' in col.lower() or 'VD3006' in col]\n    if education_cols:\n        df_viz['Educacao_Nivel'] = df_viz[education_cols[0]]\n        # Simplify education levels for clarity\n        education_mapping = {1: 'Sem Instru\u00e7\u00e3o', 2: 'Fundamental', 3: 'M\u00e9dio', 4: 'Superior', 5: 'P\u00f3s-Gradua\u00e7\u00e3o'}\n        if df_viz['Educacao_Nivel'].dtype in ['int64', 'float64']:\n            df_viz['Educacao_Nivel'] = df_viz['Educacao_Nivel'].fillna(1).astype(int).map(education_mapping).fillna('N\u00e3o Informado')\n    else:\n        # Create synthetic education data\n        df_viz['Educacao_Nivel'] = np.random.choice(['Fundamental', 'M\u00e9dio', 'Superior'], len(df_viz), \n                                                   p=[0.4, 0.4, 0.2])\n    \n    # Create flow data from education to age group to income\n    flow_data = []\n    \n    # Education -> Age Group flows\n    edu_age_flows = df_viz.groupby(['Educacao_Nivel', 'Idade_Grupo']).size().reset_index(name='count')\n    for _, row in edu_age_flows.iterrows():\n        flow_data.append({\n            'source': row['Educacao_Nivel'],\n            'target': row['Idade_Grupo'], \n            'value': row['count']\n        })\n    \n    # Age Group -> Income flows\n    age_income_flows = df_viz.groupby(['Idade_Grupo', 'Faixa_Renda']).size().reset_index(name='count')\n    for _, row in age_income_flows.iterrows():\n        flow_data.append({\n            'source': row['Idade_Grupo'],\n            'target': row['Faixa_Renda'],\n            'value': row['count']\n        })\n    \n    # Create unique node list\n    all_nodes = list(set([d['source'] for d in flow_data] + [d['target'] for d in flow_data]))\n    node_indices = {node: i for i, node in enumerate(all_nodes)}\n    \n    # Prepare data for Sankey\n    source_indices = [node_indices[d['source']] for d in flow_data]\n    target_indices = [node_indices[d['target']] for d in flow_data]\n    values = [d['value'] for d in flow_data]\n    \n    # Define colors for different categories\n    colors = {\n        # Education colors (blues)\n        'Sem Instru\u00e7\u00e3o': '#08306b', 'Fundamental': '#08519c', 'M\u00e9dio': '#3182bd', 'Superior': '#6baed6',\n        # Age group colors (greens)\n        'Jovens (0-24)': '#00441b', 'Adultos Jovens (25-34)': '#006d2c', 'Adultos (35-44)': '#31a354',\n        'Maduros (45-54)': '#74c476', 'Seniores (55+)': '#bae4b3',\n        # Income colors (reds)\n        'Muito Baixa': '#67000d', 'Baixa': '#a50f15', 'M\u00e9dia': '#cb181d', 'Alta': '#fb6a4a', 'Muito Alta': '#fcae91'\n    }\n    \n    node_colors = [colors.get(node, '#cccccc') for node in all_nodes]\n    link_colors = ['rgba(135,206,250,0.4)' for _ in flow_data]\n    \n    # Create Sankey diagram\n    fig = go.Figure(data=[go.Sankey(\n        node=dict(\n            pad=15,\n            thickness=20,\n            line=dict(color=\"black\", width=0.5),\n            label=all_nodes,\n            color=node_colors\n        ),\n        link=dict(\n            source=source_indices,\n            target=target_indices,\n            value=values,\n            color=link_colors\n        )\n    )])\n    \n    fig.update_layout(\n        title_text=\"Fluxo de Renda: Educa\u00e7\u00e3o \u2192 Idade \u2192 Faixa de Renda (PNADC)\",\n        title_x=0.5,\n        font_size=12,\n        height=600,\n        font_family=\"Arial\"\n    )\n    \n    return fig, flow_data\n\n# Create Sankey diagram\nfig_sankey, sankey_data = create_income_sankey_diagram(df)\nfig_sankey.show()\n\nprint(f\"\\n\ud83c\udf0a Fluxos de renda identificados: {len(sankey_data)} conex\u00f5es\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3n9py94om6w",
   "source": "# 3. Time Series Visualization of Income Trends by Quarter\ndef create_income_time_series(df):\n    \"\"\"\n    Create time series visualization showing income trends by quarter.\n    Note: This assumes your data contains temporal information.\n    \"\"\"\n    # Check if we have temporal data\n    temporal_cols = [col for col in df.columns if 'trim' in col.lower() or 'quarter' in col.lower() or 'ano' in col.lower()]\n    \n    if not temporal_cols:\n        print(\"\u26a0\ufe0f  No temporal columns found. Creating a synthetic time series based on available data.\")\n        # Create synthetic quarterly data based on income distribution\n        df_viz = df.copy()\n        df_viz['Trimestre_Simulado'] = np.random.choice(['2023-Q1', '2023-Q2', '2023-Q3', '2023-Q4'], len(df))\n    else:\n        # Use actual temporal data if available\n        df_viz = df.copy()\n        print(f\"\ud83d\udcc5 Found temporal columns: {temporal_cols}\")\n        # Use the first temporal column found\n        df_viz['Trimestre'] = df_viz[temporal_cols[0]]\n    \n    # Calculate quarterly statistics\n    quarter_col = 'Trimestre_Simulado' if not temporal_cols else 'Trimestre'\n    quarterly_stats = df_viz.groupby(quarter_col)[INCOME_COL].agg([\n        'mean', 'median', 'std', 'count',\n        lambda x: x.quantile(0.25),\n        lambda x: x.quantile(0.75)\n    ]).reset_index()\n    \n    quarterly_stats.columns = [quarter_col, 'Media', 'Mediana', 'DesvPadrao', 'Observacoes', 'Q1', 'Q3']\n    \n    # Create the time series plot\n    fig = go.Figure()\n    \n    # Add median line\n    fig.add_trace(go.Scatter(\n        x=quarterly_stats[quarter_col],\n        y=quarterly_stats['Mediana'],\n        mode='lines+markers',\n        name='Renda Mediana',\n        line=dict(color='#2E86AB', width=3),\n        marker=dict(size=8)\n    ))\n    \n    # Add mean line\n    fig.add_trace(go.Scatter(\n        x=quarterly_stats[quarter_col],\n        y=quarterly_stats['Media'],\n        mode='lines+markers',\n        name='Renda M\u00e9dia',\n        line=dict(color='#A23B72', width=3, dash='dash'),\n        marker=dict(size=8)\n    ))\n    \n    # Add confidence bands (Q1 to Q3)\n    fig.add_trace(go.Scatter(\n        x=quarterly_stats[quarter_col].tolist() + quarterly_stats[quarter_col].tolist()[::-1],\n        y=quarterly_stats['Q1'].tolist() + quarterly_stats['Q3'].tolist()[::-1],\n        fill='tonexty',\n        fillcolor='rgba(46,134,171,0.2)',\n        line=dict(color='rgba(255,255,255,0)'),\n        hoverinfo=\"skip\",\n        showlegend=True,\n        name='Faixa Interquartil'\n    ))\n    \n    fig.update_layout(\n        title='Evolu\u00e7\u00e3o da Renda ao Longo dos Trimestres (PNADC)',\n        title_x=0.5,\n        title_font_size=16,\n        xaxis_title='Trimestre',\n        yaxis_title='Renda (Sal\u00e1rios M\u00ednimos)',\n        height=500,\n        font_family=\"Arial\",\n        hovermode='x unified',\n        legend=dict(\n            yanchor=\"top\",\n            y=0.99,\n            xanchor=\"left\",\n            x=0.01\n        )\n    )\n    \n    # Add annotations for key insights\n    max_median_idx = quarterly_stats['Mediana'].idxmax()\n    fig.add_annotation(\n        x=quarterly_stats.iloc[max_median_idx][quarter_col],\n        y=quarterly_stats.iloc[max_median_idx]['Mediana'],\n        text=f\"Maior mediana<br>{quarterly_stats.iloc[max_median_idx]['Mediana']:.2f} SM\",\n        showarrow=True,\n        arrowhead=2,\n        arrowcolor=\"#2E86AB\",\n        bgcolor=\"white\",\n        bordercolor=\"#2E86AB\"\n    )\n    \n    return fig, quarterly_stats\n\n# Create time series visualization\nfig_timeseries, quarterly_data = create_income_time_series(df)\nfig_timeseries.show()\n\nprint(\"\\n\ud83d\udcc8 Estat\u00edsticas trimestrais:\")\nprint(quarterly_data)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5z3txicront",
   "source": "# 2. Demographic Pyramids with Income Overlays\ndef create_income_demographic_pyramids(df):\n    \"\"\"\n    Create demographic pyramids showing population distribution by age and sex,\n    with income overlays using color coding.\n    \"\"\"\n    # Prepare age groups and income brackets\n    df_viz = df.copy()\n    df_viz['Idade_Grupo'] = pd.cut(df_viz['V2009'], \n                                   bins=[0, 18, 25, 35, 45, 55, 65, 100], \n                                   labels=['0-17', '18-24', '25-34', '35-44', '45-54', '55-64', '65+'])\n    \n    # Create income brackets\n    income_percentiles = df_viz[INCOME_COL].quantile([0.25, 0.5, 0.75, 0.9]).values\n    df_viz['Faixa_Renda'] = pd.cut(df_viz[INCOME_COL], \n                                   bins=[0] + list(income_percentiles) + [df_viz[INCOME_COL].max()],\n                                   labels=['Baixa (Q1)', 'M\u00e9dia-Baixa (Q2)', 'M\u00e9dia (Q3)', 'M\u00e9dia-Alta (Q4)', 'Alta (Q5)'])\n    \n    # Group data for pyramid\n    pyramid_data = df_viz.groupby(['Idade_Grupo', 'V2007', 'Faixa_Renda']).size().reset_index(name='Count')\n    pyramid_data = pyramid_data.pivot_table(index=['Idade_Grupo', 'Faixa_Renda'], \n                                           columns='V2007', \n                                           values='Count', \n                                           fill_value=0).reset_index()\n    \n    # Create the demographic pyramid\n    fig = make_subplots(\n        rows=1, cols=2,\n        subplot_titles=('Homens', 'Mulheres'),\n        shared_yaxes=True,\n        horizontal_spacing=0.05\n    )\n    \n    colors = px.colors.qualitative.Set3[:5]  # Colors for income brackets\n    \n    for i, income_bracket in enumerate(['Baixa (Q1)', 'M\u00e9dia-Baixa (Q2)', 'M\u00e9dia (Q3)', 'M\u00e9dia-Alta (Q4)', 'Alta (Q5)']):\n        subset = pyramid_data[pyramid_data['Faixa_Renda'] == income_bracket]\n        \n        # Men (left side, negative values)\n        fig.add_trace(\n            go.Bar(\n                x=-subset['Homem'],\n                y=subset['Idade_Grupo'],\n                name=income_bracket,\n                orientation='h',\n                marker_color=colors[i],\n                hovertemplate='%{y}<br>Homens: %{x:,.0f}<extra></extra>',\n                showlegend=(i == 0)\n            ),\n            row=1, col=1\n        )\n        \n        # Women (right side, positive values)\n        fig.add_trace(\n            go.Bar(\n                x=subset['Mulher'],\n                y=subset['Idade_Grupo'],\n                name=income_bracket,\n                orientation='h',\n                marker_color=colors[i],\n                hovertemplate='%{y}<br>Mulheres: %{x:,.0f}<extra></extra>',\n                showlegend=False\n            ),\n            row=1, col=2\n        )\n    \n    fig.update_layout(\n        title='Pir\u00e2mide Demogr\u00e1fica com Sobreposi\u00e7\u00e3o de Renda (PNADC)',\n        title_x=0.5,\n        title_font_size=16,\n        height=500,\n        bargap=0.1,\n        font_family=\"Arial\",\n        legend=dict(\n            orientation=\"h\",\n            yanchor=\"bottom\",\n            y=1.02,\n            xanchor=\"right\",\n            x=1\n        )\n    )\n    \n    fig.update_xaxes(title_text=\"Popula\u00e7\u00e3o\", row=1, col=1)\n    fig.update_xaxes(title_text=\"Popula\u00e7\u00e3o\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Grupos de Idade\", row=1, col=1)\n    \n    return fig\n\n# Create demographic pyramids\nfig_pyramid = create_income_demographic_pyramids(df)\nfig_pyramid.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "k856ydb0is",
   "source": "# 1. Interactive Choropleth Map: Median Income by State\ndef create_income_choropleth(df, brazil_geojson=None):\n    \"\"\"\n    Create an interactive choropleth map showing median income by Brazilian state.\n    \"\"\"\n    # Calculate median income by state\n    state_income = df.groupby('UF_label')[INCOME_COL].agg(['median', 'mean', 'count']).reset_index()\n    state_income.columns = ['Estado', 'Renda_Mediana_SM', 'Renda_Media_SM', 'Observacoes']\n    \n    # Convert to minimum wage values for better interpretation\n    state_income['Renda_Mediana_Reais'] = state_income['Renda_Mediana_SM'] * 1320  # Approximate 2023 minimum wage\n    state_income['Renda_Media_Reais'] = state_income['Renda_Media_SM'] * 1320\n    \n    print(\"\ud83d\udcca Renda mediana por estado (em sal\u00e1rios m\u00ednimos):\")\n    print(state_income.sort_values('Renda_Mediana_SM', ascending=False).head(10))\n    \n    if brazil_geojson:\n        # Create choropleth with real geographical data\n        fig = px.choropleth(\n            state_income,\n            geojson=brazil_geojson,\n            locations='Estado',\n            color='Renda_Mediana_SM',\n            hover_name='Estado',\n            hover_data={\n                'Renda_Mediana_SM': ':.2f',\n                'Renda_Media_SM': ':.2f',\n                'Renda_Mediana_Reais': ':,.0f',\n                'Observacoes': ':,d'\n            },\n            color_continuous_scale='RdYlGn',\n            title='Renda Mediana por Estado Brasileiro (PNADC)',\n            labels={\n                'Renda_Mediana_SM': 'Renda Mediana (SM)',\n                'Estado': 'Estado'\n            }\n        )\n    else:\n        # Create a simple bar chart as fallback\n        fig = px.bar(\n            state_income.sort_values('Renda_Mediana_SM', ascending=True),\n            x='Renda_Mediana_SM',\n            y='Estado',\n            orientation='h',\n            color='Renda_Mediana_SM',\n            color_continuous_scale='RdYlGn',\n            title='Renda Mediana por Estado Brasileiro (PNADC)',\n            labels={'Renda_Mediana_SM': 'Renda Mediana (Sal\u00e1rios M\u00ednimos)'},\n            hover_data=['Renda_Media_SM', 'Observacoes']\n        )\n    \n    fig.update_layout(\n        title_font_size=16,\n        title_x=0.5,\n        height=600,\n        font_family=\"Arial\",\n        coloraxis_colorbar_title=\"Sal\u00e1rios<br>M\u00ednimos\"\n    )\n    \n    return fig, state_income\n\n# Create the choropleth map\nfig_choropleth, state_income_summary = create_income_choropleth(df, brazil_geojson)\nfig_choropleth.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "wp1bkbga9ic",
   "source": "# Fetch Brazil States GeoJSON for Choropleth Map\nimport requests\nimport json\n\ndef fetch_brazil_geojson():\n    \"\"\"Fetch Brazil states GeoJSON data from a reliable source.\"\"\"\n    url = \"https://raw.githubusercontent.com/holtzy/D3-graph-gallery/master/DATA/world.geojson\"\n    try:\n        # Try to get Brazil-specific GeoJSON\n        brazil_url = \"https://servicodados.ibge.gov.br/api/v3/malhas/paises/BR?formato=application/vnd.geo+json&qualidade=maxima&intrarregiao=uf\"\n        response = requests.get(brazil_url, timeout=10)\n        if response.status_code == 200:\n            return response.json()\n        else:\n            # Fallback to a simpler Brazil GeoJSON\n            fallback_url = \"https://raw.githubusercontent.com/codeforamerica/click_that_hood/master/public/data/brazil-states.geojson\"\n            response = requests.get(fallback_url, timeout=10)\n            if response.status_code == 200:\n                return response.json()\n    except:\n        pass\n    \n    # If all else fails, create a simple mock structure\n    print(\"Warning: Using simplified state boundaries. Consider downloading proper GeoJSON data.\")\n    return None\n\n# Fetch the GeoJSON data\nbrazil_geojson = fetch_brazil_geojson()\nprint(\"\u2705 Brazil GeoJSON data loaded successfully!\" if brazil_geojson else \"\u26a0\ufe0f  Using fallback for map data\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ztr5ddzs7x",
   "source": "# Advanced Data Visualization Setup\n# Install additional required packages for professional visualizations\nimport subprocess\nimport sys\n\n# Install additional visualization packages if not present\npackages_to_install = [\n    'plotly>=5.0',\n    'altair>=5.0', \n    'geopandas>=0.14.0',\n    'folium>=0.15.0',\n    'requests>=2.25.0'\n]\n\nfor package in packages_to_install:\n    try:\n        __import__(package.split('>=')[0])\n    except ImportError:\n        print(f\"Installing {package}...\")\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n\n# Import visualization libraries\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport altair as alt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom matplotlib.colors import LinearSegmentedColormap\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configure plotting\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\nsns.set_style(\"whitegrid\")\nalt.data_transformers.enable('json')\n\nprint(\"\u2705 Visualization libraries loaded successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}